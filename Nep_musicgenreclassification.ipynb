{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#less go\nimport os\nos.getcwd()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-11T04:58:09.527886Z","iopub.execute_input":"2023-02-11T04:58:09.528273Z","iopub.status.idle":"2023-02-11T04:58:09.536958Z","shell.execute_reply.started":"2023-02-11T04:58:09.528239Z","shell.execute_reply":"2023-02-11T04:58:09.535932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('les go')","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:58:11.046832Z","iopub.execute_input":"2023-02-11T04:58:11.047216Z","iopub.status.idle":"2023-02-11T04:58:11.052742Z","shell.execute_reply.started":"2023-02-11T04:58:11.047183Z","shell.execute_reply":"2023-02-11T04:58:11.051743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:58:12.019701Z","iopub.execute_input":"2023-02-11T04:58:12.020509Z","iopub.status.idle":"2023-02-11T04:58:22.675237Z","shell.execute_reply.started":"2023-02-11T04:58:12.020460Z","shell.execute_reply":"2023-02-11T04:58:22.674061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wandb login 31c7358e23c8522a5aece87ff899a31a639ef7d4\nimport wandb\nwandb.init(project=\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:58:44.234400Z","iopub.execute_input":"2023-02-11T04:58:44.235339Z","iopub.status.idle":"2023-02-11T04:58:53.401843Z","shell.execute_reply.started":"2023-02-11T04:58:44.235303Z","shell.execute_reply":"2023-02-11T04:58:53.400908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/facebookresearch/WavAugment\n!pip install torchaudio-augmentations\n\n# !wget http://opihi.cs.uvic.ca/sound/genres.tar.gz\n# !wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/train_filtered.txt\n# !wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/valid_filtered.txt\n# !wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/test_filtered.txt    \n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:58:59.151449Z","iopub.execute_input":"2023-02-11T04:58:59.151839Z","iopub.status.idle":"2023-02-11T04:59:17.995935Z","shell.execute_reply.started":"2023-02-11T04:58:59.151807Z","shell.execute_reply":"2023-02-11T04:59:17.994693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rename\ndef rname(fname):\n    dirnfile= fname.split('/')\n    directory = dirnfile[0]\n    f = dirnfile[1].split('.')\n    \n#     print(f)\n    file = f'{f[0]}_{f[0]}_{f[1]}.wav'\n#     print(file)\n    return file\n    \n#edit filtered\n# with open('/kaggle/working/train_filtered.txt') as f:\n#     lines = f.readlines()\n#     song_list = [line.strip() for line in lines]\n# #     rname(song_list[0])\n#     song_list_cleaned = map(rname, song_list)\n# #     print(list(song_list_cleaned))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:59:21.967911Z","iopub.execute_input":"2023-02-11T04:59:21.968404Z","iopub.status.idle":"2023-02-11T04:59:21.976934Z","shell.execute_reply.started":"2023-02-11T04:59:21.968360Z","shell.execute_reply":"2023-02-11T04:59:21.975145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport numpy as np\nimport soundfile as sf\nimport librosa\nfrom torch.utils import data\nfrom torchaudio_augmentations import (\nRandomResizedCrop,\nRandomApply,\nPolarityInversion,\nNoise,\nGain,\nHighLowPass,\nDelay,\nPitchShift,\nReverb,\nCompose,\n)\n\n\nGTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\nNEP_GENRES = ['classic','aadhunik','bhajan']\n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:59:22.546797Z","iopub.execute_input":"2023-02-11T04:59:22.547251Z","iopub.status.idle":"2023-02-11T04:59:24.785805Z","shell.execute_reply.started":"2023-02-11T04:59:22.547214Z","shell.execute_reply":"2023-02-11T04:59:24.784530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sf.available_formats()","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:59:25.322128Z","iopub.execute_input":"2023-02-11T04:59:25.325609Z","iopub.status.idle":"2023-02-11T04:59:25.333127Z","shell.execute_reply.started":"2023-02-11T04:59:25.325563Z","shell.execute_reply":"2023-02-11T04:59:25.331549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GTZANDataset(data.Dataset):\n    def __init__(self, data_path, split, num_samples, num_chunks, is_augmentation):\n        self.data_path = data_path if data_path else ''\n        self.split = split\n        self.num_samples = num_samples\n        self.num_chunks = num_chunks\n        self.is_augmentation = is_augmentation\n        self.genres = GTZAN_GENRES\n        self.buffer = None\n        self._get_song_list()\n        \n        if is_augmentation:\n            self._get_augmentations()\n        \n    def _get_song_list(self):\n        list_filename = f'/kaggle/working/{self.split}_filtered.txt'\n        \n        with open(list_filename) as f:\n            lines = f.readlines()\n            song_list = [line.strip() for line in lines]\n            rname(song_list[0])\n            song_list_cleaned = map(rname, song_list)\n#             print(list(song_list_cleaned))\n            self.song_list = list(song_list_cleaned)\n        \n#         with open(list_filename) as f:\n#             lines = f.readlines()\n#         self.song_list = [line.strip() for line in lines]\n#         print(self.song_list)\n            \n    def _get_augmentations(self):\n        transforms = [\n            RandomResizedCrop(n_samples=self.num_samples),\n            RandomApply([PolarityInversion()], p=0.8),\n            RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n            RandomApply([Gain()], p=0.2),\n            RandomApply([HighLowPass(sample_rate=22050)], p=0.8),\n            RandomApply([Delay(sample_rate=22050)], p=0.5),\n            RandomApply([PitchShift(n_samples=self.num_samples, sample_rate=22050)], p=0.4),\n            RandomApply([Reverb(sample_rate=22050)], p=0.3),\n        ]\n        self.augmentation = Compose(transforms=transforms)\n            \n    def _adjust_audio_length(self, wav):\n        if self.split == 'train':\n            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n            wav = wav[random_index : random_index + self.num_samples]\n        else:\n            hop = (len(wav) - self.num_samples) // self.num_chunks\n            wav = np.array([wav[i*hop: i*hop + self.num_samples] for i in range(self.num_chunks)])\n        return wav\n        \n    def __getitem__(self, index):\n        line= self.song_list[index]\n\n#         print(line)\n        #get genre\n        \n        genre_name = line.split('_')[0]\n#         print(genre_name)\n        genre_index = self.genres.index(genre_name)\n        \n        #get audio\n        audio_filename= os.path.join(self.data_path, 'genres_original',genre_name, line)\n#         print(audio_filename)\n#         wav, fs = sf.read(audio_filename)\n        try:\n            wav, fs = librosa.load(audio_filename)\n            \n            #adjust audio length\n            wav = self._adjust_audio_length(wav).astype('float32')\n\n            #data augmentation\n            if self.is_augmentation:\n                wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0))\n            \n            self.buffer = (wav, genre_index)\n            \n            return wav, genre_index\n        \n        \n        except:\n            \n            print(\"bad file; return buffer file to batch\")\n            return self.buffer[0], self.buffer[1]\n            \n    \n#         except:\n#             print(\"bad file error\")\n#             return None\n        \n    def __len__(self):\n        return len(self.song_list)    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:59:25.781012Z","iopub.execute_input":"2023-02-11T04:59:25.781519Z","iopub.status.idle":"2023-02-11T04:59:25.808333Z","shell.execute_reply.started":"2023-02-11T04:59:25.781481Z","shell.execute_reply":"2023-02-11T04:59:25.807023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NepDataset(data.Dataset):\n    def __init__(self, data_path, split, num_samples, num_chunks, is_augmentation):\n        self.data_path = data_path if data_path else ''\n        self.split = split\n        self.num_samples = num_samples\n        self.num_chunks = num_chunks\n        self.is_augmentation = is_augmentation\n        self.genres = NEP_GENRES\n        self.buffer = None\n        self._get_song_list()\n        \n        if is_augmentation:\n            self._get_augmentations()\n        \n    def _get_song_list(self):\n        list_filename = f'/kaggle/input/nepali-music-genre-classification/Nep_Dataset/Nep_Dataset/{self.split}.txt'\n        \n        with open(list_filename) as f:\n            lines = f.readlines()\n            song_list = [line.strip() for line in lines]\n#             rname(song_list[0])\n#             song_list_cleaned = map(rname, song_list)\n#             print(list(song_list_cleaned))\n            print(song_list)\n            self.song_list = list(song_list)\n        \n#         with open(list_filename) as f:\n#             lines = f.readlines()\n#         self.song_list = [line.strip() for line in lines]\n#         print(self.song_list)\n            \n    def _get_augmentations(self):\n        transforms = [\n            RandomResizedCrop(n_samples=self.num_samples),\n            RandomApply([PolarityInversion()], p=0.8),\n            RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n            RandomApply([Gain()], p=0.2),\n            RandomApply([HighLowPass(sample_rate=22050)], p=0.8),\n            RandomApply([Delay(sample_rate=22050)], p=0.5),\n            RandomApply([PitchShift(n_samples=self.num_samples, sample_rate=22050)], p=0.4),\n            RandomApply([Reverb(sample_rate=22050)], p=0.3),\n        ]\n        self.augmentation = Compose(transforms=transforms)\n            \n    def _adjust_audio_length(self, wav):\n        if self.split == 'train':\n            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n            wav = wav[random_index : random_index + self.num_samples]\n        else:\n            hop = (len(wav) - self.num_samples) // self.num_chunks\n            wav = np.array([wav[i*hop: i*hop + self.num_samples] for i in range(self.num_chunks)])\n        return wav\n        \n    def __getitem__(self, index):\n        line= self.song_list[index]\n\n#         print(line)\n        #get genre\n        \n        genre_name = line.split('_')[0]\n#         print(genre_name)\n        genre_index = self.genres.index(genre_name.split('/')[0])\n        \n        #get audio\n        audio_filename= os.path.join(self.data_path, line)\n        \n#         wav, fs = sf.read(audio_filename)\n        try:\n            wav, fs = librosa.load(audio_filename)\n#             print(audio_filename)\n            #adjust audio length\n            wav = self._adjust_audio_length(wav).astype('float32')\n            \n            #data augmentation\n            if self.is_augmentation:\n                wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0))\n        \n            self.buffer = (wav, genre_index)\n            return wav, genre_index\n        \n        except:\n            \n            print(\"bad file; return buffer file to batch\")\n            return self.buffer[0], self.buffer[1]\n            \n    \n#         except:\n#             print(\"bad file error\")\n#             return None\n        \n    def __len__(self):\n        return len(self.song_list)    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:00:58.755432Z","iopub.execute_input":"2023-02-11T05:00:58.755821Z","iopub.status.idle":"2023-02-11T05:00:58.773811Z","shell.execute_reply.started":"2023-02-11T05:00:58.755790Z","shell.execute_reply":"2023-02-11T05:00:58.772616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n#     print(batch)\n    return torch.utils.data.dataloader.default_collate(batch)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:00:59.450154Z","iopub.execute_input":"2023-02-11T05:00:59.450519Z","iopub.status.idle":"2023-02-11T05:00:59.458365Z","shell.execute_reply.started":"2023-02-11T05:00:59.450490Z","shell.execute_reply":"2023-02-11T05:00:59.457283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #test to check wav lengths\n# buffer = None\n# for f in os.listdir('/kaggle/input/nepali-music-genre-classification/Nep_Dataset/aadhunik/'):\n#     try:\n#         print(f)\n#         wav, fs = librosa.load('/kaggle/input/nepali-music-genre-classification/Nep_Dataset/aadhunik/'+f)\n#         #adjust audio length\n# #         wav = self._adjust_audio_length(wav).astype('float32')\n\n#         #data augmentation\n# #         if self.is_augmentation:\n# #             wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0))\n\n#         buffer = (wav, fs)\n\n#         print(len(wav), fs)\n        \n        \n#     except:\n\n#         print(\"bad file; return buffer file to batch\", f)\n#         print (buffer[0], buffer[1])\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:00:59.888549Z","iopub.execute_input":"2023-02-11T05:00:59.888916Z","iopub.status.idle":"2023-02-11T05:00:59.893819Z","shell.execute_reply.started":"2023-02-11T05:00:59.888884Z","shell.execute_reply":"2023-02-11T05:00:59.892752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataloader\ndef get_dataloader(data_path='/kaggle/input/nepali-music-genre-classification/Nep_Dataset/Nep_Dataset',\n                  split='train',\n                  num_samples=22050*29,\n                  num_chunks=1,\n#                   batch_size=16,\n                   batch_size=1,\n                  num_workers=0,\n                  is_augmentation=False):\n    is_shuffle = True if (split == 'train') else False\n    \n    batch_size = batch_size if (split=='train') else (batch_size//num_chunks)\n    data_loader = data.DataLoader(dataset=NepDataset(data_path,\n                                                      split,\n                                                      num_samples,\n                                                      num_chunks,\n                                                      is_augmentation),\n                                 batch_size = batch_size,\n                                 shuffle=is_shuffle,\n                                 drop_last=False,\n                                 num_workers=num_workers)\n    \n    return data_loader\n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:00.545621Z","iopub.execute_input":"2023-02-11T05:01:00.545990Z","iopub.status.idle":"2023-02-11T05:01:00.553944Z","shell.execute_reply.started":"2023-02-11T05:01:00.545959Z","shell.execute_reply":"2023-02-11T05:01:00.552669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = get_dataloader(split='train', is_augmentation=True)\niter_train_loader = iter(train_loader)\ntrain_wav, train_genre = next(iter_train_loader)\n\nvalid_loader = get_dataloader(split='valid')\ntest_loader = get_dataloader(split='test')\niter_test_loader = iter(test_loader)\n\ntest_wav, test_genre = next(iter_test_loader)\nprint('training data shape: %s' %str(train_wav.shape))\nprint('validation/test data shape: %s' %str(test_wav.shape))\nprint(train_genre)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:03.860201Z","iopub.execute_input":"2023-02-11T05:01:03.860562Z","iopub.status.idle":"2023-02-11T05:01:08.364140Z","shell.execute_reply.started":"2023-02-11T05:01:03.860531Z","shell.execute_reply":"2023-02-11T05:01:08.362774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\nfrom torch import nn\n\nclass Conv_2d(nn.Module):\n    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.1):\n        super(Conv_2d, self).__init__()\n        self.conv = nn.Conv2d(input_channels, output_channels, shape, padding=shape//2)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(pooling)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, wav):\n        out= self.conv(wav)\n        out= self.bn(out)\n        out= self.relu(out)\n        out= self.maxpool(out)\n        out= self.dropout(out)\n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:14.749143Z","iopub.execute_input":"2023-02-11T05:01:14.750278Z","iopub.status.idle":"2023-02-11T05:01:14.759297Z","shell.execute_reply.started":"2023-02-11T05:01:14.750235Z","shell.execute_reply":"2023-02-11T05:01:14.758124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n# import torchvision.transforms as T\n\n# def save_test_spec(testing):\n# # testing = torch.randn((16,1,128,1249))\n# #     print(testing.shape)\n#     stripped_test = testing.squeeze(1)\n# #     print(stripped_test[0].shape)\n#     new = stripped_test[0]\n#     new = new.unsqueeze(0)\n# # print(new.shape)\n#     transform = T.ToPILImage()\n#     img = transform(new)\n#     img.save('/kaggle/working/test.png')","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:15.340373Z","iopub.execute_input":"2023-02-11T05:01:15.341093Z","iopub.status.idle":"2023-02-11T05:01:15.345747Z","shell.execute_reply.started":"2023-02-11T05:01:15.341043Z","shell.execute_reply":"2023-02-11T05:01:15.344564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchaudio\n\nclass CNN(nn.Module):\n    def __init__(self, num_channels=16,\n              sample_rate = 22050,\n              n_fft=1024,\n              f_min=0.0,\n              f_max=11025.0,\n              num_mels=128,\n              num_classes=10):\n        \n        super(CNN,self).__init__()\n        \n        #mel spectrogram\n        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n                                                           n_fft=n_fft,\n                                                           f_min=f_min,\n                                                           f_max=f_max,\n                                                           n_mels=num_mels) \n        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()  #cepstral coefficients\n        self.input_bn = nn.BatchNorm2d(1)\n        \n        #convolutional layers\n        self.layer1 = Conv_2d(1, num_channels, pooling=(2,3))\n        self.layer2 = Conv_2d(num_channels, num_channels, pooling=(3,4))\n        self.layer3 = Conv_2d(num_channels, num_channels*2, pooling=(2,5))\n        self.layer4 = Conv_2d(num_channels*2, num_channels*2, pooling=(3,3))\n        self.layer5 = Conv_2d(num_channels*2, num_channels*4, pooling=(3,4))\n        \n        #dense layers\n        self.dense1 = nn.Linear(num_channels*4, num_channels*4)\n        self.dense_bn = nn.BatchNorm1d(num_channels*4)\n        self.dense2 = nn.Linear(num_channels*4, num_classes) #maybe try softmax here\n        self.dropout = nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n        \n        \n    def forward(self,wav):\n        #input processing\n        \n#         print(wav.shape)\n        out = self.melspec(wav)\n#         print(out.shape)\n#         copy = torch.clone(out)\n#         copy.to('cpu')\n#         save_test_spec(copy)\n        \n        out = self.amplitude_to_db(out)\n        #input batch\n#       out = out.unsqueeze(1)\n#         print(out.shape)\n#         out = torch.permute(out, (0,3,2,1))\n#         print(out.shape)\n        out = self.input_bn(out)\n#         print(out.shape)\n\n        #convlayers\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n\n        # reshape. (batch_size, num_channels, 1, 1) -> (batch_size, num_channels)\n        out = out.reshape(len(out), -1)\n\n        #dense layers\n        out = self.dense1(out)\n        out = self.dense_bn(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.dense2(out)\n        \n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:15.947585Z","iopub.execute_input":"2023-02-11T05:01:15.947998Z","iopub.status.idle":"2023-02-11T05:01:15.960568Z","shell.execute_reply.started":"2023-02-11T05:01:15.947967Z","shell.execute_reply":"2023-02-11T05:01:15.959345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transform for resnet18 model\nimport torchaudio\nfrom PIL import Image\nfrom torchvision import transforms\n\n# input_image = Image.open(filename)\n# preprocess = transforms.Compose([\n#     transforms.Resize((224, 1249)),\n# #     transforms.CenterCrop(224),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])\n# input_tensor = preprocess(input_image)\n# input_batch = input_tensor.unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:18.719897Z","iopub.execute_input":"2023-02-11T05:01:18.720293Z","iopub.status.idle":"2023-02-11T05:01:18.945616Z","shell.execute_reply.started":"2023-02-11T05:01:18.720263Z","shell.execute_reply":"2023-02-11T05:01:18.944641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets try resnet\n\nclass CNN_modified(nn.Module):\n    def __init__(self, num_channels=16,\n              sample_rate = 22050,\n              n_fft=1024,\n              f_min=0.0,\n              f_max=11025.0,\n              num_mels=128,\n              num_classes=10):\n        \n        super(CNN_modified,self).__init__()\n        \n        #mel spectrogram\n        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n                                                           n_fft=n_fft,\n                                                           f_min=f_min,\n                                                           f_max=f_max,\n                                                           n_mels=num_mels) \n        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()  #cepstral coefficients\n        self.input_bn = nn.BatchNorm2d(1)\n        self.resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n        self.resnet_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False) #changed no of input channels\n\n        \n        #convolutional layers\n#         self.layer1 = Conv_2d(1, num_channels, pooling=(2,3))\n#         self.layer2 = Conv_2d(num_channels, num_channels, pooling=(3,4))\n#         self.layer3 = Conv_2d(num_channels, num_channels*2, pooling=(2,5))\n#         self.layer4 = Conv_2d(num_channels*2, num_channels*2, pooling=(3,3))\n#         self.layer5 = Conv_2d(num_channels*2, num_channels*4, pooling=(3,4))\n#         self.layer1 = self.resnet_model()\n    \n        #dense layers\n#         self.dense1 = nn.Linear(num_channels*4, num_channels*4)\n#         self.dense_bn = nn.BatchNorm1d(num_channels*4)\n#         self.dense2 = nn.Linear(num_channels*4, num_classes) #maybe try softmax here\n#         self.dropout = nn.Dropout(0.5)\n#         self.relu = nn.ReLU()\n        \n        \n    def forward(self,wav):\n        #input processing\n        \n#         print(wav.shape)\n        out = self.melspec(wav)\n#         print(out.shape)\n#         copy = torch.clone(out)\n#         copy.to('cpu')\n#         save_test_spec(copy)\n        out = self.amplitude_to_db(out)\n        #input batch\n#       out = out.unsqueeze(1)\n#         print(out.shape)\n#         out = torch.permute(out, (0,3,2,1))\n#         print(out.shape)\n        out = self.input_bn(out)\n#         print(out.shape)\n        \n        \n        \n        #convlayers\n        print(out.shape)\n        out = self.resnet_model(out)\n#         out = self.layer2(out)\n#         out = self.layer3(out)\n#         out = self.layer4(out)\n#         out = self.layer5(out)\n#         out = self.resnet_model(out) #trying directly without normalizing inputs or the shape of image\n        \n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:20.345617Z","iopub.execute_input":"2023-02-11T05:01:20.345990Z","iopub.status.idle":"2023-02-11T05:01:20.358324Z","shell.execute_reply.started":"2023-02-11T05:01:20.345959Z","shell.execute_reply":"2023-02-11T05:01:20.357265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# cnn = CNN().to(device)\ncnn = CNN_modified().to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\nvalid_losses = []\nnum_epochs = 70","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:01:22.553075Z","iopub.execute_input":"2023-02-11T05:01:22.553487Z","iopub.status.idle":"2023-02-11T05:01:29.239888Z","shell.execute_reply.started":"2023-02-11T05:01:22.553456Z","shell.execute_reply":"2023-02-11T05:01:29.238884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nfor epoch in range(num_epochs):\n    train_losses = []\n    \n    #Train\n    cnn.train()\n    for (wav, genre_index) in train_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n\n        #forward\n        out= cnn(wav)\n        loss= loss_function(out, genre_index)\n\n        #backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n        mean_train_losses = np.mean(train_losses)\n    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, mean_train_losses))\n    \n    \n    #validation\n    cnn.eval()\n    y_true = []\n    y_pred = []\n    valid_losses = []\n    \n    for wav, genre_index in valid_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n\n        #reshape and aggregate chunk-level predictions\n        b, c, t = wav.size()\n        \n        check = wav.view(-1,t)\n#         print(check.shape)\n        \n#         logits = cnn(wav.view(-1,t))\n        logits = cnn(wav)\n        logits = logits.view(b, c, -1).mean(dim=1)\n        loss = loss_function(logits, genre_index)\n        valid_losses.append(loss.item())\n        _, pred = torch.max(logits.data,1)\n\n\n        #append\n        y_true.extend(genre_index.tolist())\n        y_pred.extend(pred.tolist())\n    accuracy = accuracy_score(y_true, y_pred)\n    valid_loss = np.mean(valid_losses)\n    print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy %.4f' % (epoch+1, num_epochs, valid_loss, accuracy))\n        \n    #save model\n    valid_losses.append(valid_loss.item())\n#     if(epoch%2 == 0):\n    name = 'model_'+ str(epoch)+'.ckpt'\n    torch.save(cnn.state_dict(),name)\n#     if np.argmin(valid_losses) == epoch:\n#         print('Saving the best model at %d epochs!' % epoch)\n#         torch.save(cnn.state_dict(), 'best_model.ckpt')\n    \n    wandb.log({\"train_loss\": mean_train_losses, \"valid_loss\":valid_loss, \"accuracy\":accuracy})\n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T04:58:31.502927Z","iopub.execute_input":"2023-02-11T04:58:31.503313Z","iopub.status.idle":"2023-02-11T04:58:31.528801Z","shell.execute_reply.started":"2023-02-11T04:58:31.503282Z","shell.execute_reply":"2023-02-11T04:58:31.527469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %debug\n#evaluation\n\nS = torch.load('/kaggle/input/nepali-music-genre-classification/Nep_model_49.ckpt')\n# print(S)\ncnn.load_state_dict(S, strict=False)\n\nprint('loaded!')\n\n#Run evaluation\n\ncnn.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for wav, genre_index in test_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n        \n        #reshape and aggregate chunk- level predictions\n        b, c, t = wav.size()\n#         print(\"b,c,t\",b,c,t)\n#         logits = cnn(wav.view(-1, t))\n        print(\"wav.view(-1,t).shape\",wav.view(-1,t).shape)\n        logits = cnn(wav)\n#         print(\"logits\", logits)\n#         logits = logits.view(b, c, -1).mean(dim=1)\n#         print(\"logits data\",logits.data)\n        _, pred = torch.max(logits.data, 1)\n#         print(\"predictions\", pred)\n        #append labels and predictions\n        y_true.extend(genre_index.tolist())\n        y_pred.extend(pred.tolist())\n#         print(y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:07:50.692986Z","iopub.execute_input":"2023-02-11T05:07:50.693687Z","iopub.status.idle":"2023-02-11T05:08:54.199091Z","shell.execute_reply.started":"2023-02-11T05:07:50.693651Z","shell.execute_reply":"2023-02-11T05:08:54.197683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# print(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, xticklabels=NEP_GENRES, yticklabels=NEP_GENRES, cmap='YlGnBu')\nprint('Accuracy: %.4f' % accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:10:28.985938Z","iopub.execute_input":"2023-02-11T05:10:28.986555Z","iopub.status.idle":"2023-02-11T05:10:29.322455Z","shell.execute_reply.started":"2023-02-11T05:10:28.986503Z","shell.execute_reply":"2023-02-11T05:10:29.321416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}