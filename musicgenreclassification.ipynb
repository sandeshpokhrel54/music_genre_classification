{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#less go","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-12T03:32:09.351758Z","iopub.execute_input":"2022-12-12T03:32:09.352115Z","iopub.status.idle":"2022-12-12T03:32:09.356717Z","shell.execute_reply.started":"2022-12-12T03:32:09.352083Z","shell.execute_reply":"2022-12-12T03:32:09.355488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('les go')","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:01.232528Z","iopub.execute_input":"2022-12-28T03:32:01.232939Z","iopub.status.idle":"2022-12-28T03:32:01.257137Z","shell.execute_reply.started":"2022-12-28T03:32:01.232838Z","shell.execute_reply":"2022-12-28T03:32:01.256144Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"les go\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:01.258931Z","iopub.execute_input":"2022-12-28T03:32:01.259274Z","iopub.status.idle":"2022-12-28T03:32:13.489046Z","shell.execute_reply.started":"2022-12-28T03:32:01.259240Z","shell.execute_reply":"2022-12-28T03:32:13.487738Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.21)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.9.10)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!wandb login 31c7358e23c8522a5aece87ff899a31a639ef7d4\nimport wandb\nwandb.init(project=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:13.492022Z","iopub.execute_input":"2022-12-28T03:32:13.492464Z","iopub.status.idle":"2022-12-28T03:32:24.607232Z","shell.execute_reply.started":"2022-12-28T03:32:13.492408Z","shell.execute_reply":"2022-12-28T03:32:24.606282Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msandeshpokhrel\u001b[0m (\u001b[33mteam-friday\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20221228_033217-1nux0hq3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/team-friday/test/runs/1nux0hq3\" target=\"_blank\">fancy-microwave-12</a></strong> to <a href=\"https://wandb.ai/team-friday/test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/team-friday/test/runs/1nux0hq3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f8d06118b10>"},"metadata":{}}]},{"cell_type":"code","source":"# !pip install git+https://github.com/facebookresearch/WavAugment\n!pip install torchaudio-augmentations\n\n# \n# !wget http://opihi.cs.uvic.ca/sound/genres.tar.gz\n!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/train_filtered.txt\n!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/valid_filtered.txt\n!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/test_filtered.txt    \n","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:24.612428Z","iopub.execute_input":"2022-12-28T03:32:24.615014Z","iopub.status.idle":"2022-12-28T03:32:47.042644Z","shell.execute_reply.started":"2022-12-28T03:32:24.614975Z","shell.execute_reply":"2022-12-28T03:32:47.041501Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting torchaudio-augmentations\n  Downloading torchaudio_augmentations-0.2.4-py3-none-any.whl (12 kB)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (from torchaudio-augmentations) (0.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchaudio-augmentations) (1.21.6)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchaudio-augmentations) (1.11.0)\nCollecting torch-pitch-shift\n  Downloading torch_pitch_shift-1.2.2-py3-none-any.whl (5.0 kB)\nCollecting wavaugment\n  Downloading wavaugment-0.2-py3-none-any.whl (5.4 kB)\nCollecting julius\n  Downloading julius-0.2.7.tar.gz (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m686.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchaudio-augmentations) (4.1.1)\nCollecting primePy>=1.3\n  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from torch-pitch-shift->torchaudio-augmentations) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->torch-pitch-shift->torchaudio-augmentations) (3.0.9)\nBuilding wheels for collected packages: julius\n  Building wheel for julius (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21894 sha256=9715509c8d6280012805505cb78eb31b8f72262f2981e0cfe8cd9bef17f49238\n  Stored in directory: /root/.cache/pip/wheels/44/52/2c/7dd069f82c7f905f40b190a8039ec2a17fdd4bb009c57c6664\nSuccessfully built julius\nInstalling collected packages: primePy, julius, wavaugment, torch-pitch-shift, torchaudio-augmentations\nSuccessfully installed julius-0.2.7 primePy-1.3 torch-pitch-shift-1.2.2 torchaudio-augmentations-0.2.4 wavaugment-0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m--2022-12-28 03:32:43--  https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/train_filtered.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10152 (9.9K) [text/plain]\nSaving to: ‘train_filtered.txt’\n\ntrain_filtered.txt  100%[===================>]   9.91K  --.-KB/s    in 0s      \n\n2022-12-28 03:32:44 (53.2 MB/s) - ‘train_filtered.txt’ saved [10152/10152]\n\n--2022-12-28 03:32:45--  https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/valid_filtered.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4522 (4.4K) [text/plain]\nSaving to: ‘valid_filtered.txt’\n\nvalid_filtered.txt  100%[===================>]   4.42K  --.-KB/s    in 0s      \n\n2022-12-28 03:32:45 (37.7 MB/s) - ‘valid_filtered.txt’ saved [4522/4522]\n\n--2022-12-28 03:32:46--  https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/test_filtered.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6616 (6.5K) [text/plain]\nSaving to: ‘test_filtered.txt’\n\ntest_filtered.txt   100%[===================>]   6.46K  --.-KB/s    in 0s      \n\n2022-12-28 03:32:46 (43.7 MB/s) - ‘test_filtered.txt’ saved [6616/6616]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#rename\ndef rname(fname):\n    dirnfile= fname.split('/')\n    directory = dirnfile[0]\n    f = dirnfile[1].split('.')\n    \n#     print(f)\n    file = f'{f[0]}_{f[0]}_{f[1]}.wav'\n#     print(file)\n    return file\n    \n#edit filtered\nwith open('/kaggle/working/train_filtered.txt') as f:\n    lines = f.readlines()\n    song_list = [line.strip() for line in lines]\n#     rname(song_list[0])\n    song_list_cleaned = map(rname, song_list)\n#     print(list(song_list_cleaned))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:47.044271Z","iopub.execute_input":"2022-12-28T03:32:47.045864Z","iopub.status.idle":"2022-12-28T03:32:47.059675Z","shell.execute_reply.started":"2022-12-28T03:32:47.045824Z","shell.execute_reply":"2022-12-28T03:32:47.058706Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport numpy as np\nimport soundfile as sf\nimport librosa\nfrom torch.utils import data\nfrom torchaudio_augmentations import (\nRandomResizedCrop,\nRandomApply,\nPolarityInversion,\nNoise,\nGain,\nHighLowPass,\nDelay,\nPitchShift,\nReverb,\nCompose,\n)\n\n\nGTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\nNEP_GENRES = ['classical','adhunik','lok dohori','filmy','bhajan']\n","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:47.063083Z","iopub.execute_input":"2022-12-28T03:32:47.063756Z","iopub.status.idle":"2022-12-28T03:32:51.055821Z","shell.execute_reply.started":"2022-12-28T03:32:47.063719Z","shell.execute_reply":"2022-12-28T03:32:51.054487Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# sf.available_formats()","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:51.060583Z","iopub.execute_input":"2022-12-28T03:32:51.061464Z","iopub.status.idle":"2022-12-28T03:32:51.072650Z","shell.execute_reply.started":"2022-12-28T03:32:51.061412Z","shell.execute_reply":"2022-12-28T03:32:51.071140Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class GTZANDataset(data.Dataset):\n    def __init__(self, data_path, split, num_samples, num_chunks, is_augmentation):\n        self.data_path = data_path if data_path else ''\n        self.split = split\n        self.num_samples = num_samples\n        self.num_chunks = num_chunks\n        self.is_augmentation = is_augmentation\n        self.genres = GTZAN_GENRES\n        self.buffer = None\n        self._get_song_list()\n        \n        if is_augmentation:\n            self._get_augmentations()\n        \n    def _get_song_list(self):\n        list_filename = f'/kaggle/working/{self.split}_filtered.txt'\n        \n        with open(list_filename) as f:\n            lines = f.readlines()\n            song_list = [line.strip() for line in lines]\n            rname(song_list[0])\n            song_list_cleaned = map(rname, song_list)\n#             print(list(song_list_cleaned))\n            self.song_list = list(song_list_cleaned)\n        \n#         with open(list_filename) as f:\n#             lines = f.readlines()\n#         self.song_list = [line.strip() for line in lines]\n#         print(self.song_list)\n            \n    def _get_augmentations(self):\n        transforms = [\n            RandomResizedCrop(n_samples=self.num_samples),\n            RandomApply([PolarityInversion()], p=0.8),\n            RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n            RandomApply([Gain()], p=0.2),\n            RandomApply([HighLowPass(sample_rate=22050)], p=0.8),\n            RandomApply([Delay(sample_rate=22050)], p=0.5),\n            RandomApply([PitchShift(n_samples=self.num_samples, sample_rate=22050)], p=0.4),\n            RandomApply([Reverb(sample_rate=22050)], p=0.3),\n        ]\n        self.augmentation = Compose(transforms=transforms)\n            \n    def _adjust_audio_length(self, wav):\n        if self.split == 'train':\n            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n            wav = wav[random_index : random_index + self.num_samples]\n        else:\n            hop = (len(wav) - self.num_samples) // self.num_chunks\n            wav = np.array([wav[i*hop: i*hop + self.num_samples] for i in range(self.num_chunks)])\n        return wav\n        \n    def __getitem__(self, index):\n        line= self.song_list[index]\n\n#         print(line)\n        #get genre\n        \n        genre_name = line.split('_')[0]\n#         print(genre_name)\n        genre_index = self.genres.index(genre_name)\n        \n        #get audio\n        audio_filename= os.path.join(self.data_path, 'genres_original',genre_name, line)\n#         print(audio_filename)\n#         wav, fs = sf.read(audio_filename)\n        try:\n            wav, fs = librosa.load(audio_filename)\n            \n            #adjust audio length\n            wav = self._adjust_audio_length(wav).astype('float32')\n\n            #data augmentation\n            if self.is_augmentation:\n                wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0))\n            \n            self.buffer = (wav, genre_index)\n            \n            return wav, genre_index\n        \n        \n        except:\n            \n            print(\"bad file; return buffer file to batch\")\n            return self.buffer[0], self.buffer[1]\n            \n    \n#         except:\n#             print(\"bad file error\")\n#             return None\n        \n    def __len__(self):\n        return len(self.song_list)    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:51.077346Z","iopub.execute_input":"2022-12-28T03:32:51.077691Z","iopub.status.idle":"2022-12-28T03:32:51.122765Z","shell.execute_reply.started":"2022-12-28T03:32:51.077658Z","shell.execute_reply":"2022-12-28T03:32:51.121196Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    return torch.utils.data.dataloader.default_collate(batch)","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:51.130007Z","iopub.execute_input":"2022-12-28T03:32:51.136623Z","iopub.status.idle":"2022-12-28T03:32:51.143142Z","shell.execute_reply.started":"2022-12-28T03:32:51.136586Z","shell.execute_reply":"2022-12-28T03:32:51.142177Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#dataloader\ndef get_dataloader(data_path='/kaggle/input/gtzanpreprocessed/Data/Data',\n                  split='train',\n                  num_samples=22050*29,\n                  num_chunks=1,\n                  batch_size=16,\n                  num_workers=0,\n                  is_augmentation=False,\n                  collate_fn = collate_fn):\n    is_shuffle = True if (split == 'train') else False\n    batch_size = batch_size if (split=='train') else (batch_size//num_chunks)\n    data_loader = data.DataLoader(dataset=GTZANDataset(data_path,\n                                                      split,\n                                                      num_samples,\n                                                      num_chunks,\n                                                      is_augmentation),\n                                 batch_size = batch_size,\n                                 shuffle=is_shuffle,\n                                 drop_last=False,\n                                 num_workers=num_workers)\n    return data_loader\n","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:51.144143Z","iopub.execute_input":"2022-12-28T03:32:51.144912Z","iopub.status.idle":"2022-12-28T03:32:51.157401Z","shell.execute_reply.started":"2022-12-28T03:32:51.144877Z","shell.execute_reply":"2022-12-28T03:32:51.156465Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_loader = get_dataloader(split='train', is_augmentation=True)\niter_train_loader = iter(train_loader)\ntrain_wav, train_genre = next(iter_train_loader)\n\nvalid_loader = get_dataloader(split='valid')\ntest_loader = get_dataloader(split='test')\niter_test_loader = iter(test_loader)\n\ntest_wav, test_genre = next(iter_test_loader)\nprint('training data shape: %s' %str(train_wav.shape))\nprint('validation/test data shape: %s' %str(test_wav.shape))\nprint(train_genre)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:51.159408Z","iopub.execute_input":"2022-12-28T03:32:51.161362Z","iopub.status.idle":"2022-12-28T03:32:56.711226Z","shell.execute_reply.started":"2022-12-28T03:32:51.161318Z","shell.execute_reply":"2022-12-28T03:32:56.710193Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"training data shape: torch.Size([16, 1, 639450])\nvalidation/test data shape: torch.Size([16, 1, 639450])\ntensor([4, 2, 7, 3, 6, 4, 5, 2, 0, 3, 9, 6, 2, 9, 5, 7])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\nfrom torch import nn\n\nclass Conv_2d(nn.Module):\n    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.1):\n        super(Conv_2d, self).__init__()\n        self.conv = nn.Conv2d(input_channels, output_channels, shape, padding=shape//2)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(pooling)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, wav):\n        out= self.conv(wav)\n        out= self.bn(out)\n        out= self.relu(out)\n        out= self.maxpool(out)\n        out= self.dropout(out)\n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:56.715642Z","iopub.execute_input":"2022-12-28T03:32:56.718225Z","iopub.status.idle":"2022-12-28T03:32:56.730704Z","shell.execute_reply.started":"2022-12-28T03:32:56.718178Z","shell.execute_reply":"2022-12-28T03:32:56.729657Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport torchvision.transforms as T\n\ndef save_test_spec(testing):\n# testing = torch.randn((16,1,128,1249))\n#     print(testing.shape)\n    stripped_test = testing.squeeze(1)\n#     print(stripped_test[0].shape)\n    new = stripped_test[0]\n    new = new.unsqueeze(0)\n# print(new.shape)\n    transform = T.ToPILImage()\n    img = transform(new)\n    img.save('/kaggle/working/test.png')","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:56.735697Z","iopub.execute_input":"2022-12-28T03:32:56.738338Z","iopub.status.idle":"2022-12-28T03:32:56.991609Z","shell.execute_reply.started":"2022-12-28T03:32:56.738300Z","shell.execute_reply":"2022-12-28T03:32:56.990613Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torchaudio\n\nclass CNN(nn.Module):\n    def __init__(self, num_channels=16,\n              sample_rate = 22050,\n              n_fft=1024,\n              f_min=0.0,\n              f_max=11025.0,\n              num_mels=128,\n              num_classes=10):\n        \n        super(CNN,self).__init__()\n        \n        #mel spectrogram\n        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n                                                           n_fft=n_fft,\n                                                           f_min=f_min,\n                                                           f_max=f_max,\n                                                           n_mels=num_mels) \n        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()  #cepstral coefficients\n        self.input_bn = nn.BatchNorm2d(1)\n        \n        #convolutional layers\n        self.layer1 = Conv_2d(1, num_channels, pooling=(2,3))\n        self.layer2 = Conv_2d(num_channels, num_channels, pooling=(3,4))\n        self.layer3 = Conv_2d(num_channels, num_channels*2, pooling=(2,5))\n        self.layer4 = Conv_2d(num_channels*2, num_channels*2, pooling=(3,3))\n        self.layer5 = Conv_2d(num_channels*2, num_channels*4, pooling=(3,4))\n        \n        #dense layers\n        self.dense1 = nn.Linear(num_channels*4, num_channels*4)\n        self.dense_bn = nn.BatchNorm1d(num_channels*4)\n        self.dense2 = nn.Linear(num_channels*4, num_classes) #maybe try softmax here\n        self.dropout = nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n        \n        \n    def forward(self,wav):\n        #input processing\n        \n#         print(wav.shape)\n        out = self.melspec(wav)\n#         print(out.shape)\n#         copy = torch.clone(out)\n#         copy.to('cpu')\n#         save_test_spec(copy)\n        \n        out = self.amplitude_to_db(out)\n        #input batch\n#       out = out.unsqueeze(1)\n#         print(out.shape)\n#         out = torch.permute(out, (0,3,2,1))\n#         print(out.shape)\n        out = self.input_bn(out)\n#         print(out.shape)\n\n        #convlayers\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n\n        # reshape. (batch_size, num_channels, 1, 1) -> (batch_size, num_channels)\n        out = out.reshape(len(out), -1)\n\n        #dense layers\n        out = self.dense1(out)\n        out = self.dense_bn(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.dense2(out)\n        \n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:56.996461Z","iopub.execute_input":"2022-12-28T03:32:56.998798Z","iopub.status.idle":"2022-12-28T03:32:57.017856Z","shell.execute_reply.started":"2022-12-28T03:32:56.998759Z","shell.execute_reply":"2022-12-28T03:32:57.016932Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#transform for resnet18 model\n\nfrom PIL import Image\nfrom torchvision import transforms\n\n# input_image = Image.open(filename)\npreprocess = transforms.Compose([\n    transforms.Resize((224, 1249)),\n#     transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n# input_tensor = preprocess(input_image)\n# input_batch = input_tensor.unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:57.022775Z","iopub.execute_input":"2022-12-28T03:32:57.025396Z","iopub.status.idle":"2022-12-28T03:32:57.034729Z","shell.execute_reply.started":"2022-12-28T03:32:57.025359Z","shell.execute_reply":"2022-12-28T03:32:57.033763Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#lets try resnet\n\nclass CNN_modified(nn.Module):\n    def __init__(self, num_channels=16,\n              sample_rate = 22050,\n              n_fft=1024,\n              f_min=0.0,\n              f_max=11025.0,\n              num_mels=128,\n              num_classes=10):\n        \n        super(CNN_modified,self).__init__()\n        \n        #mel spectrogram\n        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n                                                           n_fft=n_fft,\n                                                           f_min=f_min,\n                                                           f_max=f_max,\n                                                           n_mels=num_mels) \n        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()  #cepstral coefficients\n        self.input_bn = nn.BatchNorm2d(1)\n        self.resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n        self.resnet_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False) #changed no of input channels\n\n        \n        #convolutional layers\n#         self.layer1 = Conv_2d(1, num_channels, pooling=(2,3))\n#         self.layer2 = Conv_2d(num_channels, num_channels, pooling=(3,4))\n#         self.layer3 = Conv_2d(num_channels, num_channels*2, pooling=(2,5))\n#         self.layer4 = Conv_2d(num_channels*2, num_channels*2, pooling=(3,3))\n#         self.layer5 = Conv_2d(num_channels*2, num_channels*4, pooling=(3,4))\n#         self.layer1 = self.resnet_model()\n    \n        #dense layers\n#         self.dense1 = nn.Linear(num_channels*4, num_channels*4)\n#         self.dense_bn = nn.BatchNorm1d(num_channels*4)\n#         self.dense2 = nn.Linear(num_channels*4, num_classes) #maybe try softmax here\n#         self.dropout = nn.Dropout(0.5)\n#         self.relu = nn.ReLU()\n        \n        \n    def forward(self,wav):\n        #input processing\n        \n#         print(wav.shape)\n        out = self.melspec(wav)\n#         print(out.shape)\n#         copy = torch.clone(out)\n#         copy.to('cpu')\n#         save_test_spec(copy)\n        \n        #make sure to reshape to at least 224x224 and normalizing with suggested standard deviation and mean\n        \n        #WRITE THAT FUNC\n        \n        out = self.amplitude_to_db(out)\n        #input batch\n#       out = out.unsqueeze(1)\n#         print(out.shape)\n#         out = torch.permute(out, (0,3,2,1))\n#         print(out.shape)\n        out = self.input_bn(out)\n#         print(out.shape)\n        \n        \n        \n        #convlayers\n        print(out.shape)\n        out = self.resnet_model(out)\n#         out = self.layer2(out)\n#         out = self.layer3(out)\n#         out = self.layer4(out)\n#         out = self.layer5(out)\n#         out = self.resnet_model(out) #trying directly without normalizing inputs or the shape of image\n        \n        return out\n","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:57.039650Z","iopub.execute_input":"2022-12-28T03:32:57.042113Z","iopub.status.idle":"2022-12-28T03:32:57.057143Z","shell.execute_reply.started":"2022-12-28T03:32:57.042076Z","shell.execute_reply":"2022-12-28T03:32:57.056203Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# cnn = CNN().to(device)\ncnn = CNN_modified().to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\nvalid_losses = []\nnum_epochs = 100","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:32:57.061922Z","iopub.execute_input":"2022-12-28T03:32:57.064640Z","iopub.status.idle":"2022-12-28T03:33:03.901346Z","shell.execute_reply.started":"2022-12-28T03:32:57.064602Z","shell.execute_reply":"2022-12-28T03:33:03.900372Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/pytorch/vision/archive/v0.10.0.zip\" to /root/.cache/torch/hub/v0.10.0.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"#training\nfor epoch in range(num_epochs):\n    train_losses = []\n    \n    #Train\n    cnn.train()\n    for (wav, genre_index) in train_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n\n        #forward\n        out= cnn(wav)\n        loss= loss_function(out, genre_index)\n\n        #backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n        mean_train_losses = np.mean(train_losses)\n    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, mean_train_losses))\n    \n    \n    #validation\n    cnn.eval()\n    y_true = []\n    y_pred = []\n    valid_losses = []\n    \n    for wav, genre_index in valid_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n\n        #reshape and aggregate chunk-level predictions\n        b, c, t = wav.size()\n        \n        check = wav.view(-1,t)\n#         print(check.shape)\n        \n#         logits = cnn(wav.view(-1,t))\n        logits = cnn(wav)\n        logits = logits.view(b, c, -1).mean(dim=1)\n        loss = loss_function(logits, genre_index)\n        valid_losses.append(loss.item())\n        _, pred = torch.max(logits.data,1)\n\n\n        #append\n        y_true.extend(genre_index.tolist())\n        y_pred.extend(pred.tolist())\n    accuracy = accuracy_score(y_true, y_pred)\n    valid_loss = np.mean(valid_losses)\n    print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy %.4f' % (epoch+1, num_epochs, valid_loss, accuracy))\n        \n    #save model\n    valid_losses.append(valid_loss.item())\n    if(epoch%5 == 0):\n        name = 'model_'+ str(epoch)+'.ckpt'\n    torch.save(cnn.state_dict(),name)\n#     if np.argmin(valid_losses) == epoch:\n#         print('Saving the best model at %d epochs!' % epoch)\n#         torch.save(cnn.state_dict(), 'best_model.ckpt')\n    \n    wandb.log({\"train_loss\": mean_train_losses, \"valid_loss\":valid_loss, \"accuracy\":accuracy})\n","metadata":{"execution":{"iopub.status.busy":"2022-12-27T05:34:31.831854Z","iopub.execute_input":"2022-12-27T05:34:31.832909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-12-27T16:20:20.967524Z","iopub.execute_input":"2022-12-27T16:20:20.967896Z","iopub.status.idle":"2022-12-27T16:20:20.973855Z","shell.execute_reply.started":"2022-12-27T16:20:20.967866Z","shell.execute_reply":"2022-12-27T16:20:20.972779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %debug\n#evaluation\n\nS = torch.load('/kaggle/input/gtzanpreprocessed/model_40.ckpt')\n# print(S)\ncnn.load_state_dict(S, strict=False)\n\nprint('loaded!')\n\n#Run evaluation\n\ncnn.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for wav, genre_index in test_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n        \n        #reshape and aggregate chunk- level predictions\n        b, c, t = wav.size()\n#         print(\"b,c,t\",b,c,t)\n#         logits = cnn(wav.view(-1, t))\n        print(\"wav.view(-1,t).shape\",wav.view(-1,t).shape)\n        logits = cnn(wav)\n        print(\"logits\", logits)\n#         logits = logits.view(b, c, -1).mean(dim=1)\n        print(\"logits data\",logits.data)\n        _, pred = torch.max(logits.data, 1)\n        print(\"predictions\", pred)\n        #append labels and predictions\n        y_true.extend(genre_index.tolist())\n        y_pred.extend(pred.tolist())\n#         print(y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:35:49.733755Z","iopub.execute_input":"2022-12-28T03:35:49.734129Z","iopub.status.idle":"2022-12-28T03:35:52.097794Z","shell.execute_reply.started":"2022-12-28T03:35:49.734098Z","shell.execute_reply":"2022-12-28T03:35:52.095855Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"loaded!\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  5.6276,  10.1088,   0.5326,  ..., -13.8730, -14.6433, -12.6785],\n        [  6.4343,   8.5896,  -0.0948,  ..., -14.3921, -15.0611, -13.3208],\n        [  7.4438,   5.0721,   1.7387,  ..., -15.0236, -15.5910, -14.4641],\n        ...,\n        [  5.9166,   8.7332,  -0.8728,  ..., -16.7917, -17.4445, -16.1245],\n        [  8.0743,   7.1470,  -0.5836,  ..., -17.0147, -17.7209, -16.5845],\n        [  7.0007,   7.5791,   2.3751,  ..., -16.3444, -17.1547, -15.6795]],\n       device='cuda:0')\nlogits data tensor([[  5.6276,  10.1088,   0.5326,  ..., -13.8730, -14.6433, -12.6785],\n        [  6.4343,   8.5896,  -0.0948,  ..., -14.3921, -15.0611, -13.3208],\n        [  7.4438,   5.0721,   1.7387,  ..., -15.0236, -15.5910, -14.4641],\n        ...,\n        [  5.9166,   8.7332,  -0.8728,  ..., -16.7917, -17.4445, -16.1245],\n        [  8.0743,   7.1470,  -0.5836,  ..., -17.0147, -17.7209, -16.5845],\n        [  7.0007,   7.5791,   2.3751,  ..., -16.3444, -17.1547, -15.6795]],\n       device='cuda:0')\npredictions tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[ 8.9620e+00,  7.2459e+00,  6.1324e-01,  ..., -1.6623e+01,\n         -1.7428e+01, -1.5670e+01],\n        [ 4.9522e+00,  2.4740e+00,  5.6121e+00,  ..., -1.0962e+01,\n         -1.2155e+01, -1.0735e+01],\n        [ 5.9500e+00,  1.8478e+00,  5.0160e+00,  ..., -1.1281e+01,\n         -1.2158e+01, -1.0805e+01],\n        ...,\n        [ 5.2094e+00, -1.5882e-02,  7.2231e+00,  ..., -1.1600e+01,\n         -1.3055e+01, -1.1407e+01],\n        [ 5.4773e+00,  7.4104e-01,  7.1202e+00,  ..., -1.1099e+01,\n         -1.2273e+01, -1.0712e+01],\n        [ 3.0509e+00,  1.0618e+01,  3.9376e+00,  ..., -1.1035e+01,\n         -1.1741e+01, -9.9093e+00]], device='cuda:0')\nlogits data tensor([[ 8.9620e+00,  7.2459e+00,  6.1324e-01,  ..., -1.6623e+01,\n         -1.7428e+01, -1.5670e+01],\n        [ 4.9522e+00,  2.4740e+00,  5.6121e+00,  ..., -1.0962e+01,\n         -1.2155e+01, -1.0735e+01],\n        [ 5.9500e+00,  1.8478e+00,  5.0160e+00,  ..., -1.1281e+01,\n         -1.2158e+01, -1.0805e+01],\n        ...,\n        [ 5.2094e+00, -1.5882e-02,  7.2231e+00,  ..., -1.1600e+01,\n         -1.3055e+01, -1.1407e+01],\n        [ 5.4773e+00,  7.4104e-01,  7.1202e+00,  ..., -1.1099e+01,\n         -1.2273e+01, -1.0712e+01],\n        [ 3.0509e+00,  1.0618e+01,  3.9376e+00,  ..., -1.1035e+01,\n         -1.1741e+01, -9.9093e+00]], device='cuda:0')\npredictions tensor([5, 2, 0, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 1], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  3.8027,  12.1279,   3.1202,  ..., -11.4113, -12.1975, -10.2812],\n        [  3.8568,  11.9610,   3.3972,  ..., -11.7268, -12.5251, -10.5221],\n        [  3.4061,  13.1121,   2.3368,  ..., -11.4767, -12.3412, -10.3935],\n        ...,\n        [  3.8638,  11.6371,   4.3114,  ..., -12.7135, -13.7476, -11.3207],\n        [  4.0363,  10.6017,   2.2386,  ..., -11.6333, -12.4963, -10.6849],\n        [  2.6959,  10.0574,   3.9100,  ..., -10.8446, -11.5818,  -9.8949]],\n       device='cuda:0')\nlogits data tensor([[  3.8027,  12.1279,   3.1202,  ..., -11.4113, -12.1975, -10.2812],\n        [  3.8568,  11.9610,   3.3972,  ..., -11.7268, -12.5251, -10.5221],\n        [  3.4061,  13.1121,   2.3368,  ..., -11.4767, -12.3412, -10.3935],\n        ...,\n        [  3.8638,  11.6371,   4.3114,  ..., -12.7135, -13.7476, -11.3207],\n        [  4.0363,  10.6017,   2.2386,  ..., -11.6333, -12.4963, -10.6849],\n        [  2.6959,  10.0574,   3.9100,  ..., -10.8446, -11.5818,  -9.8949]],\n       device='cuda:0')\npredictions tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  3.7061,  11.5751,   4.0916,  ..., -12.3390, -13.3237, -10.9971],\n        [  3.7904,  13.6104,   3.2980,  ..., -12.3317, -13.3584, -11.1958],\n        [  3.7761,  13.3274,   3.7968,  ..., -12.4072, -13.3207, -11.0104],\n        ...,\n        [  3.2165,   9.8520,   2.1228,  ...,  -9.9943, -10.7552,  -9.0659],\n        [  4.1956,   5.6889,   6.7173,  ..., -11.6672, -12.3239, -10.5111],\n        [  2.4646,   2.5227,   8.2927,  ..., -11.7857, -12.6741, -10.9060]],\n       device='cuda:0')\nlogits data tensor([[  3.7061,  11.5751,   4.0916,  ..., -12.3390, -13.3237, -10.9971],\n        [  3.7904,  13.6104,   3.2980,  ..., -12.3317, -13.3584, -11.1958],\n        [  3.7761,  13.3274,   3.7968,  ..., -12.4072, -13.3207, -11.0104],\n        ...,\n        [  3.2165,   9.8520,   2.1228,  ...,  -9.9943, -10.7552,  -9.0659],\n        [  4.1956,   5.6889,   6.7173,  ..., -11.6672, -12.3239, -10.5111],\n        [  2.4646,   2.5227,   8.2927,  ..., -11.7857, -12.6741, -10.9060]],\n       device='cuda:0')\npredictions tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  5.0873,   3.1201,   7.8464,  ..., -12.0456, -12.8358, -10.9804],\n        [  5.8270,   2.3262,   9.2631,  ..., -13.1659, -14.2082, -12.0521],\n        [  3.3581,  -1.9851,   4.2502,  ..., -12.0517, -12.9292, -11.5521],\n        ...,\n        [  6.5382,  -0.9547,  10.5593,  ..., -13.2843, -14.4043, -12.2742],\n        [  5.2738,   1.5154,   8.9461,  ..., -12.3346, -13.4378, -11.7302],\n        [  9.2839,   0.7264,  10.9860,  ..., -12.5396, -13.8124, -11.9904]],\n       device='cuda:0')\nlogits data tensor([[  5.0873,   3.1201,   7.8464,  ..., -12.0456, -12.8358, -10.9804],\n        [  5.8270,   2.3262,   9.2631,  ..., -13.1659, -14.2082, -12.0521],\n        [  3.3581,  -1.9851,   4.2502,  ..., -12.0517, -12.9292, -11.5521],\n        ...,\n        [  6.5382,  -0.9547,  10.5593,  ..., -13.2843, -14.4043, -12.2742],\n        [  5.2738,   1.5154,   8.9461,  ..., -12.3346, -13.4378, -11.7302],\n        [  9.2839,   0.7264,  10.9860,  ..., -12.5396, -13.8124, -11.9904]],\n       device='cuda:0')\npredictions tensor([2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 5, 2, 2, 2], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  4.7591,   0.3612,   9.7660,  ..., -12.5711, -14.0004, -12.0540],\n        [  6.1025,   2.9010,  10.9677,  ..., -12.6875, -13.8972, -11.9118],\n        [  5.1924,   3.0295,   9.0067,  ..., -11.9401, -13.2230, -11.2479],\n        ...,\n        [  2.9124,  -0.3963,   1.0141,  ..., -10.8342, -11.6955, -10.3400],\n        [  5.0970,  -2.5151,   6.7156,  ..., -12.3080, -13.5184, -12.0982],\n        [  6.7694,   0.1609,   1.8876,  ..., -11.2158, -11.9945, -11.0804]],\n       device='cuda:0')\nlogits data tensor([[  4.7591,   0.3612,   9.7660,  ..., -12.5711, -14.0004, -12.0540],\n        [  6.1025,   2.9010,  10.9677,  ..., -12.6875, -13.8972, -11.9118],\n        [  5.1924,   3.0295,   9.0067,  ..., -11.9401, -13.2230, -11.2479],\n        ...,\n        [  2.9124,  -0.3963,   1.0141,  ..., -10.8342, -11.6955, -10.3400],\n        [  5.0970,  -2.5151,   6.7156,  ..., -12.3080, -13.5184, -12.0982],\n        [  6.7694,   0.1609,   1.8876,  ..., -11.2158, -11.9945, -11.0804]],\n       device='cuda:0')\npredictions tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 8], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  3.3323,  -4.8868,  -1.6190,  ..., -15.5626, -16.1376, -15.4970],\n        [  7.0024,   1.1057,  -2.0599,  ..., -13.7414, -13.9125, -13.0378],\n        [  6.5814,   1.4949,   4.9264,  ..., -10.6422, -11.5353, -10.4213],\n        ...,\n        [  3.4308,  -0.3857,   1.6245,  ..., -11.4130, -11.9847, -11.0513],\n        [  4.0029,  -3.4117,  -3.4804,  ..., -15.4797, -15.5594, -15.3637],\n        [  5.4497,  -0.5430,   7.5400,  ..., -11.9364, -13.0919, -11.5787]],\n       device='cuda:0')\nlogits data tensor([[  3.3323,  -4.8868,  -1.6190,  ..., -15.5626, -16.1376, -15.4970],\n        [  7.0024,   1.1057,  -2.0599,  ..., -13.7414, -13.9125, -13.0378],\n        [  6.5814,   1.4949,   4.9264,  ..., -10.6422, -11.5353, -10.4213],\n        ...,\n        [  3.4308,  -0.3857,   1.6245,  ..., -11.4130, -11.9847, -11.0513],\n        [  4.0029,  -3.4117,  -3.4804,  ..., -15.4797, -15.5594, -15.3637],\n        [  5.4497,  -0.5430,   7.5400,  ..., -11.9364, -13.0919, -11.5787]],\n       device='cuda:0')\npredictions tensor([4, 8, 0, 3, 3, 3, 3, 3, 3, 2, 8, 3, 8, 8, 4, 2], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  4.2574,  -1.7085,   6.7532,  ..., -11.7379, -12.9133, -11.1963],\n        [  4.0301,   0.7223,   5.2792,  ..., -12.9341, -14.2367, -12.4006],\n        [  7.1792,   3.0447,   3.8164,  ..., -11.5341, -12.3656, -10.6044],\n        ...,\n        [  3.4917,  -1.3126,   5.9366,  ..., -12.5573, -13.1712, -11.9722],\n        [  4.0174,  -2.7499,   3.0034,  ..., -11.4566, -11.9300, -11.3585],\n        [  3.7859,   2.9365,   2.4964,  ..., -12.0520, -12.6067, -11.1311]],\n       device='cuda:0')\nlogits data tensor([[  4.2574,  -1.7085,   6.7532,  ..., -11.7379, -12.9133, -11.1963],\n        [  4.0301,   0.7223,   5.2792,  ..., -12.9341, -14.2367, -12.4006],\n        [  7.1792,   3.0447,   3.8164,  ..., -11.5341, -12.3656, -10.6044],\n        ...,\n        [  3.4917,  -1.3126,   5.9366,  ..., -12.5573, -13.1712, -11.9722],\n        [  4.0174,  -2.7499,   3.0034,  ..., -11.4566, -11.9300, -11.3585],\n        [  3.7859,   2.9365,   2.4964,  ..., -12.0520, -12.6067, -11.1311]],\n       device='cuda:0')\npredictions tensor([3, 3, 0, 0, 3, 2, 8, 8, 3, 4, 4, 4, 3, 7, 7, 5], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  4.7878,  -3.4743,  -1.3270,  ..., -16.5435, -17.0058, -16.7583],\n        [  5.3939,  -2.9937,   2.5926,  ..., -11.0425, -11.7957, -10.8782],\n        [  3.0928,  -2.7200,  -2.1875,  ..., -13.0523, -13.3920, -13.0655],\n        ...,\n        [  3.7861,  -3.9246,   1.5806,  ..., -11.0350, -11.6767, -11.0415],\n        [  9.6968,  -2.4056,   3.6778,  ..., -14.0761, -15.1325, -13.7537],\n        [  4.1596,  -2.8782,   3.5088,  ..., -12.6038, -13.4511, -12.4544]],\n       device='cuda:0')\nlogits data tensor([[  4.7878,  -3.4743,  -1.3270,  ..., -16.5435, -17.0058, -16.7583],\n        [  5.3939,  -2.9937,   2.5926,  ..., -11.0425, -11.7957, -10.8782],\n        [  3.0928,  -2.7200,  -2.1875,  ..., -13.0523, -13.3920, -13.0655],\n        ...,\n        [  3.7861,  -3.9246,   1.5806,  ..., -11.0350, -11.6767, -11.0415],\n        [  9.6968,  -2.4056,   3.6778,  ..., -14.0761, -15.1325, -13.7537],\n        [  4.1596,  -2.8782,   3.5088,  ..., -12.6038, -13.4511, -12.4544]],\n       device='cuda:0')\npredictions tensor([4, 8, 4, 8, 4, 8, 3, 4, 4, 3, 4, 4, 4, 7, 0, 4], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  3.9875,  -4.6505,   4.2874,  ..., -11.4693, -12.2913, -11.4469],\n        [  3.6498,  -4.7425,   2.7130,  ..., -12.4024, -12.9905, -12.8811],\n        [  3.9707,  -4.7361,  -1.2308,  ..., -14.6957, -15.0736, -14.9365],\n        ...,\n        [  3.5267,   5.3524,   3.6389,  ..., -11.2421, -11.9314, -10.0867],\n        [  4.7946,   2.4494,   3.1842,  ..., -10.9502, -11.5976, -10.1040],\n        [  3.2719,   1.7615,   2.0661,  ..., -10.6870, -11.0106, -10.2280]],\n       device='cuda:0')\nlogits data tensor([[  3.9875,  -4.6505,   4.2874,  ..., -11.4693, -12.2913, -11.4469],\n        [  3.6498,  -4.7425,   2.7130,  ..., -12.4024, -12.9905, -12.8811],\n        [  3.9707,  -4.7361,  -1.2308,  ..., -14.6957, -15.0736, -14.9365],\n        ...,\n        [  3.5267,   5.3524,   3.6389,  ..., -11.2421, -11.9314, -10.0867],\n        [  4.7946,   2.4494,   3.1842,  ..., -10.9502, -11.5976, -10.1040],\n        [  3.2719,   1.7615,   2.0661,  ..., -10.6870, -11.0106, -10.2280]],\n       device='cuda:0')\npredictions tensor([7, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  3.5134,   2.5122,   2.6786,  ..., -11.5491, -12.1098, -10.9267],\n        [  2.3164,   3.1782,   3.8597,  ..., -10.5806, -11.2168,  -9.9389],\n        [  6.0223,   2.0927,   3.4309,  ...,  -9.3737,  -9.8717,  -8.7900],\n        ...,\n        [  7.3995,   5.5851,   1.0409,  ..., -13.3374, -14.2846, -13.0966],\n        [  7.2885,   4.4451,   2.2617,  ..., -12.5674, -13.5545, -12.3418],\n        [  2.0427,  -0.5369,   0.9491,  ..., -12.4247, -14.2110, -13.1324]],\n       device='cuda:0')\nlogits data tensor([[  3.5134,   2.5122,   2.6786,  ..., -11.5491, -12.1098, -10.9267],\n        [  2.3164,   3.1782,   3.8597,  ..., -10.5806, -11.2168,  -9.9389],\n        [  6.0223,   2.0927,   3.4309,  ...,  -9.3737,  -9.8717,  -8.7900],\n        ...,\n        [  7.3995,   5.5851,   1.0409,  ..., -13.3374, -14.2846, -13.0966],\n        [  7.2885,   4.4451,   2.2617,  ..., -12.5674, -13.5545, -12.3418],\n        [  2.0427,  -0.5369,   0.9491,  ..., -12.4247, -14.2110, -13.1324]],\n       device='cuda:0')\npredictions tensor([5, 7, 0, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 6], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  1.3824,  -1.8585,   0.7307,  ..., -12.7654, -14.4238, -13.4140],\n        [  3.6546,  -1.2296,   2.5076,  ..., -12.5483, -14.4705, -13.2167],\n        [  3.7727,  -1.4368,   6.3847,  ..., -12.2717, -14.0017, -12.7841],\n        ...,\n        [  4.3770,  -1.1461,   1.6157,  ..., -11.2407, -12.9563, -12.0730],\n        [  3.7950,  -2.1545,   4.9476,  ..., -12.8277, -14.5852, -13.3134],\n        [  5.7559,  -0.7038,   5.7204,  ..., -10.5573, -12.0308, -10.7884]],\n       device='cuda:0')\nlogits data tensor([[  1.3824,  -1.8585,   0.7307,  ..., -12.7654, -14.4238, -13.4140],\n        [  3.6546,  -1.2296,   2.5076,  ..., -12.5483, -14.4705, -13.2167],\n        [  3.7727,  -1.4368,   6.3847,  ..., -12.2717, -14.0017, -12.7841],\n        ...,\n        [  4.3770,  -1.1461,   1.6157,  ..., -11.2407, -12.9563, -12.0730],\n        [  3.7950,  -2.1545,   4.9476,  ..., -12.8277, -14.5852, -13.3134],\n        [  5.7559,  -0.7038,   5.7204,  ..., -10.5573, -12.0308, -10.7884]],\n       device='cuda:0')\npredictions tensor([6, 6, 6, 2, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 9], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  2.7286,  -0.7632,   6.2352,  ..., -12.9415, -14.6807, -13.2304],\n        [  1.8286,  -2.4350,   3.0972,  ..., -10.6147, -11.4767, -10.6915],\n        [  4.5809,  -2.0683,   4.6484,  ..., -10.9036, -12.4566, -11.3100],\n        ...,\n        [  4.4694,  -1.3172,   6.1624,  ..., -10.3250, -11.3179, -10.1004],\n        [  5.1500,  -3.9145,   1.4532,  ..., -13.1744, -13.7080, -13.1312],\n        [  3.4192,   3.4553,   4.3449,  ..., -10.8044, -11.4525,  -9.9173]],\n       device='cuda:0')\nlogits data tensor([[  2.7286,  -0.7632,   6.2352,  ..., -12.9415, -14.6807, -13.2304],\n        [  1.8286,  -2.4350,   3.0972,  ..., -10.6147, -11.4767, -10.6915],\n        [  4.5809,  -2.0683,   4.6484,  ..., -10.9036, -12.4566, -11.3100],\n        ...,\n        [  4.4694,  -1.3172,   6.1624,  ..., -10.3250, -11.3179, -10.1004],\n        [  5.1500,  -3.9145,   1.4532,  ..., -13.1744, -13.7080, -13.1312],\n        [  3.4192,   3.4553,   4.3449,  ..., -10.8044, -11.4525,  -9.9173]],\n       device='cuda:0')\npredictions tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 7, 2, 8, 7], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  2.4640,  -1.1523,  -1.8391,  ..., -12.0833, -12.3520, -11.8074],\n        [  5.5036,   0.4675,   6.6965,  ..., -11.1002, -11.9580, -10.6009],\n        [  3.1138,   0.6867,   3.3820,  ..., -12.7957, -13.3933, -12.1978],\n        ...,\n        [  4.4807,   3.8254,   3.4927,  ..., -10.6374, -11.2691,  -9.6732],\n        [  2.7620,  -3.3810,   0.1396,  ..., -10.8965, -11.7503, -10.9476],\n        [  1.8135,  -4.8583,  -0.9762,  ..., -13.0204, -13.5033, -12.9606]],\n       device='cuda:0')\nlogits data tensor([[  2.4640,  -1.1523,  -1.8391,  ..., -12.0833, -12.3520, -11.8074],\n        [  5.5036,   0.4675,   6.6965,  ..., -11.1002, -11.9580, -10.6009],\n        [  3.1138,   0.6867,   3.3820,  ..., -12.7957, -13.3933, -12.1978],\n        ...,\n        [  4.4807,   3.8254,   3.4927,  ..., -10.6374, -11.2691,  -9.6732],\n        [  2.7620,  -3.3810,   0.1396,  ..., -10.8965, -11.7503, -10.9476],\n        [  1.8135,  -4.8583,  -0.9762,  ..., -13.0204, -13.5033, -12.9606]],\n       device='cuda:0')\npredictions tensor([7, 2, 7, 6, 7, 7, 3, 7, 7, 7, 7, 4, 7, 5, 7, 7], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  1.7901,  -3.2148,  -1.3074,  ..., -12.1933, -12.6878, -12.1614],\n        [  4.3061,  -0.7446,   3.7379,  ..., -12.3558, -13.4253, -11.8151],\n        [  3.4190,  -4.6744,  -0.1342,  ..., -12.5173, -13.3125, -12.7254],\n        ...,\n        [  4.8857,  -0.7108,  -0.6057,  ..., -11.9849, -11.8756, -11.3466],\n        [  6.4302,   0.8690,   5.2024,  ..., -10.8684, -11.8799, -10.4129],\n        [  6.9592,  -0.3246,   7.6923,  ..., -11.4306, -12.4652, -10.9338]],\n       device='cuda:0')\nlogits data tensor([[  1.7901,  -3.2148,  -1.3074,  ..., -12.1933, -12.6878, -12.1614],\n        [  4.3061,  -0.7446,   3.7379,  ..., -12.3558, -13.4253, -11.8151],\n        [  3.4190,  -4.6744,  -0.1342,  ..., -12.5173, -13.3125, -12.7254],\n        ...,\n        [  4.8857,  -0.7108,  -0.6057,  ..., -11.9849, -11.8756, -11.3466],\n        [  6.4302,   0.8690,   5.2024,  ..., -10.8684, -11.8799, -10.4129],\n        [  6.9592,  -0.3246,   7.6923,  ..., -11.4306, -12.4652, -10.9338]],\n       device='cuda:0')\npredictions tensor([7, 7, 7, 7, 3, 7, 7, 7, 8, 8, 8, 8, 8, 8, 0, 2], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  6.1400,  -1.6761,   6.0190,  ..., -12.5544, -13.2715, -12.2476],\n        [  6.7203,  -1.1528,   5.3263,  ..., -12.9649, -13.4764, -12.5511],\n        [  2.4279,  -3.9623,  -1.9477,  ..., -11.9623, -12.2314, -11.8692],\n        ...,\n        [  6.4343,   1.4771,   4.0900,  ..., -12.3663, -12.9431, -11.3056],\n        [  4.6452,  -0.8243,  -0.4112,  ..., -12.9933, -13.1308, -12.4505],\n        [  3.5195,  -1.7645,   0.9239,  ..., -11.3368, -11.8409, -10.9076]],\n       device='cuda:0')\nlogits data tensor([[  6.1400,  -1.6761,   6.0190,  ..., -12.5544, -13.2715, -12.2476],\n        [  6.7203,  -1.1528,   5.3263,  ..., -12.9649, -13.4764, -12.5511],\n        [  2.4279,  -3.9623,  -1.9477,  ..., -11.9623, -12.2314, -11.8692],\n        ...,\n        [  6.4343,   1.4771,   4.0900,  ..., -12.3663, -12.9431, -11.3056],\n        [  4.6452,  -0.8243,  -0.4112,  ..., -12.9933, -13.1308, -12.4505],\n        [  3.5195,  -1.7645,   0.9239,  ..., -11.3368, -11.8409, -10.9076]],\n       device='cuda:0')\npredictions tensor([0, 8, 4, 8, 8, 8, 8, 2, 8, 8, 8, 4, 7, 0, 8, 3], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  3.8238,   2.5782,  -0.7458,  ..., -12.2658, -12.3720, -11.6116],\n        [  3.5752,  -0.1026,   0.0943,  ..., -10.6200, -10.7874, -10.2426],\n        [  4.7115,  -1.5925,   6.8625,  ..., -10.8060, -12.0866, -10.6891],\n        ...,\n        [  4.1633,   1.0960,   6.2024,  ..., -10.6860, -11.7054, -10.0722],\n        [  3.1148,   3.9289,   5.4823,  ..., -10.7782, -11.5295, -10.0251],\n        [  4.5831,   1.4459,   6.9516,  ..., -11.2835, -12.3595, -10.4252]],\n       device='cuda:0')\nlogits data tensor([[  3.8238,   2.5782,  -0.7458,  ..., -12.2658, -12.3720, -11.6116],\n        [  3.5752,  -0.1026,   0.0943,  ..., -10.6200, -10.7874, -10.2426],\n        [  4.7115,  -1.5925,   6.8625,  ..., -10.8060, -12.0866, -10.6891],\n        ...,\n        [  4.1633,   1.0960,   6.2024,  ..., -10.6860, -11.7054, -10.0722],\n        [  3.1148,   3.9289,   5.4823,  ..., -10.7782, -11.5295, -10.0251],\n        [  4.5831,   1.4459,   6.9516,  ..., -11.2835, -12.3595, -10.4252]],\n       device='cuda:0')\npredictions tensor([8, 8, 2, 0, 0, 8, 8, 8, 9, 9, 9, 2, 3, 2, 2, 2], device='cuda:0')\nwav.view(-1,t).shape torch.Size([16, 639450])\ntorch.Size([16, 1, 128, 1249])\nlogits tensor([[  2.5194,  -0.0187,   3.3858,  ..., -10.1103, -10.8149,  -9.5425],\n        [  3.1355,  -1.1497,   3.6559,  ...,  -9.9881, -11.2876, -10.0409],\n        [  2.6998,   1.1050,   5.7596,  ..., -10.9647, -12.2010, -10.2862],\n        ...,\n        [  2.9996,   0.1562,   4.7137,  ..., -11.3475, -12.5283, -11.0398],\n        [  4.5671,   2.7568,   6.6073,  ..., -11.6853, -13.2078, -10.9868],\n        [  1.9619,  -0.3213,   2.4652,  ..., -10.3062, -11.4116, -10.4499]],\n       device='cuda:0')\nlogits data tensor([[  2.5194,  -0.0187,   3.3858,  ..., -10.1103, -10.8149,  -9.5425],\n        [  3.1355,  -1.1497,   3.6559,  ...,  -9.9881, -11.2876, -10.0409],\n        [  2.6998,   1.1050,   5.7596,  ..., -10.9647, -12.2010, -10.2862],\n        ...,\n        [  2.9996,   0.1562,   4.7137,  ..., -11.3475, -12.5283, -11.0398],\n        [  4.5671,   2.7568,   6.6073,  ..., -11.6853, -13.2078, -10.9868],\n        [  1.9619,  -0.3213,   2.4652,  ..., -10.3062, -11.4116, -10.4499]],\n       device='cuda:0')\npredictions tensor([9, 7, 9, 0, 3, 3, 3, 3, 7, 2, 3, 6, 9, 9, 9, 6], device='cuda:0')\nwav.view(-1,t).shape torch.Size([2, 639450])\ntorch.Size([2, 1, 128, 1249])\nlogits tensor([[  7.0618,   1.0170,   9.3862,  ..., -12.9387, -14.2301, -12.3105],\n        [  6.2863,   2.2557,   5.7893,  ..., -11.5151, -12.6691, -11.1048]],\n       device='cuda:0')\nlogits data tensor([[  7.0618,   1.0170,   9.3862,  ..., -12.9387, -14.2301, -12.3105],\n        [  6.2863,   2.2557,   5.7893,  ..., -11.5151, -12.6691, -11.1048]],\n       device='cuda:0')\npredictions tensor([2, 0], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# print(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, xticklabels=GTZAN_GENRES, yticklabels=GTZAN_GENRES, cmap='YlGnBu')\nprint('Accuracy: %.4f' % accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-12-28T03:35:52.099866Z","iopub.execute_input":"2022-12-28T03:35:52.100208Z","iopub.status.idle":"2022-12-28T03:35:52.735508Z","shell.execute_reply.started":"2022-12-28T03:35:52.100173Z","shell.execute_reply":"2022-12-28T03:35:52.734388Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Accuracy: 0.6207\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}