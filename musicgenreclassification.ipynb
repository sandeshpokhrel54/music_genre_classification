{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#less go","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-12T03:32:09.351758Z","iopub.execute_input":"2022-12-12T03:32:09.352115Z","iopub.status.idle":"2022-12-12T03:32:09.356717Z","shell.execute_reply.started":"2022-12-12T03:32:09.352083Z","shell.execute_reply":"2022-12-12T03:32:09.355488Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print('les go')","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:01:36.071775Z","iopub.execute_input":"2022-12-12T04:01:36.072174Z","iopub.status.idle":"2022-12-12T04:01:36.101148Z","shell.execute_reply.started":"2022-12-12T04:01:36.072082Z","shell.execute_reply":"2022-12-12T04:01:36.100319Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"les go\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:01:36.106593Z","iopub.execute_input":"2022-12-12T04:01:36.107215Z","iopub.status.idle":"2022-12-12T04:01:48.209634Z","shell.execute_reply.started":"2022-12-12T04:01:36.107177Z","shell.execute_reply":"2022-12-12T04:01:48.208474Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.21)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.9.10)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.4)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# import wandb\n# wandb.init(project=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:34:53.181511Z","iopub.execute_input":"2022-12-12T03:34:53.182253Z","iopub.status.idle":"2022-12-12T03:34:53.186388Z","shell.execute_reply.started":"2022-12-12T03:34:53.182213Z","shell.execute_reply":"2022-12-12T03:34:53.185391Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/facebookresearch/WavAugment\n!pip install torchaudio-augmentations\n\n# \n# !wget http://opihi.cs.uvic.ca/sound/genres.tar.gz\n!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/train_filtered.txt\n!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/valid_filtered.txt\n!wget https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/test_filtered.txt    \n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:01:48.212981Z","iopub.execute_input":"2022-12-12T04:01:48.214021Z","iopub.status.idle":"2022-12-12T04:02:03.262572Z","shell.execute_reply.started":"2022-12-12T04:01:48.213977Z","shell.execute_reply":"2022-12-12T04:02:03.261447Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torchaudio-augmentations\n  Downloading torchaudio_augmentations-0.2.4-py3-none-any.whl (12 kB)\nCollecting julius\n  Downloading julius-0.2.7.tar.gz (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting wavaugment\n  Downloading wavaugment-0.2-py3-none-any.whl (5.4 kB)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (from torchaudio-augmentations) (0.11.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchaudio-augmentations) (1.11.0)\nCollecting torch-pitch-shift\n  Downloading torch_pitch_shift-1.2.2-py3-none-any.whl (5.0 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchaudio-augmentations) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchaudio-augmentations) (4.1.1)\nCollecting primePy>=1.3\n  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from torch-pitch-shift->torchaudio-augmentations) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->torch-pitch-shift->torchaudio-augmentations) (3.0.9)\nBuilding wheels for collected packages: julius\n  Building wheel for julius (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21894 sha256=c1d09b02eb46efb5dfdf60d53c57cf4745dced25dbc92f9183ba4b0a9108835a\n  Stored in directory: /root/.cache/pip/wheels/44/52/2c/7dd069f82c7f905f40b190a8039ec2a17fdd4bb009c57c6664\nSuccessfully built julius\nInstalling collected packages: primePy, julius, wavaugment, torch-pitch-shift, torchaudio-augmentations\nSuccessfully installed julius-0.2.7 primePy-1.3 torch-pitch-shift-1.2.2 torchaudio-augmentations-0.2.4 wavaugment-0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m--2022-12-12 04:02:00--  https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/train_filtered.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10152 (9.9K) [text/plain]\nSaving to: ‘train_filtered.txt’\n\ntrain_filtered.txt  100%[===================>]   9.91K  --.-KB/s    in 0s      \n\n2022-12-12 04:02:00 (33.8 MB/s) - ‘train_filtered.txt’ saved [10152/10152]\n\n--2022-12-12 04:02:01--  https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/valid_filtered.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4522 (4.4K) [text/plain]\nSaving to: ‘valid_filtered.txt’\n\nvalid_filtered.txt  100%[===================>]   4.42K  --.-KB/s    in 0s      \n\n2022-12-12 04:02:02 (35.4 MB/s) - ‘valid_filtered.txt’ saved [4522/4522]\n\n--2022-12-12 04:02:03--  https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/test_filtered.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6616 (6.5K) [text/plain]\nSaving to: ‘test_filtered.txt’\n\ntest_filtered.txt   100%[===================>]   6.46K  --.-KB/s    in 0s      \n\n2022-12-12 04:02:03 (56.1 MB/s) - ‘test_filtered.txt’ saved [6616/6616]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#rename\ndef rname(fname):\n    dirnfile= fname.split('/')\n    directory = dirnfile[0]\n    f = dirnfile[1].split('.')\n    \n#     print(f)\n    file = f'{f[0]}_{f[0]}_{f[1]}.wav'\n#     print(file)\n    return file\n    \n#edit filtered\nwith open('/kaggle/working/train_filtered.txt') as f:\n    lines = f.readlines()\n    song_list = [line.strip() for line in lines]\n#     rname(song_list[0])\n    song_list_cleaned = map(rname, song_list)\n#     print(list(song_list_cleaned))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:02:03.265690Z","iopub.execute_input":"2022-12-12T04:02:03.266002Z","iopub.status.idle":"2022-12-12T04:02:03.273017Z","shell.execute_reply.started":"2022-12-12T04:02:03.265971Z","shell.execute_reply":"2022-12-12T04:02:03.272065Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport numpy as np\nimport soundfile as sf\nimport librosa\nfrom torch.utils import data\nfrom torchaudio_augmentations import (\nRandomResizedCrop,\nRandomApply,\nPolarityInversion,\nNoise,\nGain,\nHighLowPass,\nDelay,\nPitchShift,\nReverb,\nCompose,\n)\n\n\nGTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\nNEP_GENRES = ['classical','adhunik','lok dohori','filmy','bhajan']\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:02:03.276159Z","iopub.execute_input":"2022-12-12T04:02:03.276906Z","iopub.status.idle":"2022-12-12T04:02:06.700629Z","shell.execute_reply.started":"2022-12-12T04:02:03.276867Z","shell.execute_reply":"2022-12-12T04:02:06.699616Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# sf.available_formats()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:35:13.879966Z","iopub.execute_input":"2022-12-12T03:35:13.882485Z","iopub.status.idle":"2022-12-12T03:35:13.889673Z","shell.execute_reply.started":"2022-12-12T03:35:13.882447Z","shell.execute_reply":"2022-12-12T03:35:13.887849Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class GTZANDataset(data.Dataset):\n    def __init__(self, data_path, split, num_samples, num_chunks, is_augmentation):\n        self.data_path = data_path if data_path else ''\n        self.split = split\n        self.num_samples = num_samples\n        self.num_chunks = num_chunks\n        self.is_augmentation = is_augmentation\n        self.genres = GTZAN_GENRES\n        self.buffer = None\n        self._get_song_list()\n        \n        if is_augmentation:\n            self._get_augmentations()\n        \n    def _get_song_list(self):\n        list_filename = f'/kaggle/working/{self.split}_filtered.txt'\n        \n        with open(list_filename) as f:\n            lines = f.readlines()\n            song_list = [line.strip() for line in lines]\n            rname(song_list[0])\n            song_list_cleaned = map(rname, song_list)\n#             print(list(song_list_cleaned))\n            self.song_list = list(song_list_cleaned)\n        \n#         with open(list_filename) as f:\n#             lines = f.readlines()\n#         self.song_list = [line.strip() for line in lines]\n#         print(self.song_list)\n            \n    def _get_augmentations(self):\n        transforms = [\n            RandomResizedCrop(n_samples=self.num_samples),\n            RandomApply([PolarityInversion()], p=0.8),\n            RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n            RandomApply([Gain()], p=0.2),\n            RandomApply([HighLowPass(sample_rate=22050)], p=0.8),\n            RandomApply([Delay(sample_rate=22050)], p=0.5),\n            RandomApply([PitchShift(n_samples=self.num_samples, sample_rate=22050)], p=0.4),\n            RandomApply([Reverb(sample_rate=22050)], p=0.3),\n        ]\n        self.augmentation = Compose(transforms=transforms)\n            \n    def _adjust_audio_length(self, wav):\n        if self.split == 'train':\n            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n            wav = wav[random_index : random_index + self.num_samples]\n        else:\n            hop = (len(wav) - self.num_samples) // self.num_chunks\n            wav = np.array([wav[i*hop: i*hop + self.num_samples] for i in range(self.num_chunks)])\n        return wav\n        \n    def __getitem__(self, index):\n        line= self.song_list[index]\n\n#         print(line)\n        #get genre\n        \n        genre_name = line.split('_')[0]\n#         print(genre_name)\n        genre_index = self.genres.index(genre_name)\n        \n        #get audio\n        audio_filename= os.path.join(self.data_path, 'genres_original',genre_name, line)\n#         print(audio_filename)\n#         wav, fs = sf.read(audio_filename)\n        try:\n            wav, fs = librosa.load(audio_filename)\n            \n            #adjust audio length\n            wav = self._adjust_audio_length(wav).astype('float32')\n\n            #data augmentation\n            if self.is_augmentation:\n                wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0))\n            \n            self.buffer = (wav, genre_index)\n            \n            return wav, genre_index\n        \n        \n        except:\n            \n            print(\"bad file; return buffer file to batch\")\n            return self.buffer[0], self.buffer[1]\n            \n    \n#         except:\n#             print(\"bad file error\")\n#             return None\n        \n    def __len__(self):\n        return len(self.song_list)    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:02:06.702435Z","iopub.execute_input":"2022-12-12T04:02:06.703020Z","iopub.status.idle":"2022-12-12T04:02:06.719216Z","shell.execute_reply.started":"2022-12-12T04:02:06.702983Z","shell.execute_reply":"2022-12-12T04:02:06.717293Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    return torch.utils.data.dataloader.default_collate(batch)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:02:06.720474Z","iopub.execute_input":"2022-12-12T04:02:06.720892Z","iopub.status.idle":"2022-12-12T04:02:06.734210Z","shell.execute_reply.started":"2022-12-12T04:02:06.720853Z","shell.execute_reply":"2022-12-12T04:02:06.733321Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#dataloader\ndef get_dataloader(data_path='/kaggle/input/gtzanpreprocessed/Data/Data',\n                  split='train',\n                  num_samples=22050*29,\n                  num_chunks=1,\n                  batch_size=16,\n                  num_workers=0,\n                  is_augmentation=False,\n                  collate_fn = collate_fn):\n    is_shuffle = True if (split == 'train') else False\n    batch_size = batch_size if (split=='train') else (batch_size//num_chunks)\n    data_loader = data.DataLoader(dataset=GTZANDataset(data_path,\n                                                      split,\n                                                      num_samples,\n                                                      num_chunks,\n                                                      is_augmentation),\n                                 batch_size = batch_size,\n                                 shuffle=is_shuffle,\n                                 drop_last=False,\n                                 num_workers=num_workers)\n    return data_loader\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:02:06.735616Z","iopub.execute_input":"2022-12-12T04:02:06.736125Z","iopub.status.idle":"2022-12-12T04:02:06.745390Z","shell.execute_reply.started":"2022-12-12T04:02:06.736086Z","shell.execute_reply":"2022-12-12T04:02:06.744392Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_loader = get_dataloader(split='train', is_augmentation=True)\niter_train_loader = iter(train_loader)\ntrain_wav, train_genre = next(iter_train_loader)\n\nvalid_loader = get_dataloader(split='valid')\ntest_loader = get_dataloader(split='test')\niter_test_loader = iter(test_loader)\n\ntest_wav, test_genre = next(iter_test_loader)\nprint('training data shape: %s' %str(train_wav.shape))\nprint('validation/test data shape: %s' %str(test_wav.shape))\nprint(train_genre)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:02:06.746687Z","iopub.execute_input":"2022-12-12T04:02:06.747136Z","iopub.status.idle":"2022-12-12T04:02:12.031357Z","shell.execute_reply.started":"2022-12-12T04:02:06.747101Z","shell.execute_reply":"2022-12-12T04:02:12.029995Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"bad file; return buffer file to batch\ntraining data shape: torch.Size([16, 1, 639450])\nvalidation/test data shape: torch.Size([16, 1, 639450])\ntensor([5, 1, 6, 5, 0, 9, 9, 8, 3, 5, 1, 5, 7, 8, 5, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\nfrom torch import nn\n\nclass Conv_2d(nn.Module):\n    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.1):\n        super(Conv_2d, self).__init__()\n        self.conv = nn.Conv2d(input_channels, output_channels, shape, padding=shape//2)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(pooling)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, wav):\n        out= self.conv(wav)\n        out= self.bn(out)\n        out= self.relu(out)\n        out= self.maxpool(out)\n        out= self.dropout(out)\n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:02:12.033436Z","iopub.execute_input":"2022-12-12T04:02:12.033846Z","iopub.status.idle":"2022-12-12T04:02:12.043877Z","shell.execute_reply.started":"2022-12-12T04:02:12.033804Z","shell.execute_reply":"2022-12-12T04:02:12.042927Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport torchvision.transforms as T\n\ndef save_test_spec(testing):\n# testing = torch.randn((16,1,128,1249))\n#     print(testing.shape)\n    stripped_test = testing.squeeze(1)\n#     print(stripped_test[0].shape)\n    new = stripped_test[0]\n    new = new.unsqueeze(0)\n# print(new.shape)\n    transform = T.ToPILImage()\n    img = transform(new)\n    img.save('/kaggle/working/test.png')","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:04:01.657751Z","iopub.execute_input":"2022-12-12T04:04:01.658133Z","iopub.status.idle":"2022-12-12T04:04:01.664766Z","shell.execute_reply.started":"2022-12-12T04:04:01.658098Z","shell.execute_reply":"2022-12-12T04:04:01.663594Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torchaudio\n\nclass CNN(nn.Module):\n    def __init__(self, num_channels=16,\n              sample_rate = 22050,\n              n_fft=1024,\n              f_min=0.0,\n              f_max=11025.0,\n              num_mels=128,\n              num_classes=10):\n        \n        super(CNN,self).__init__()\n        \n        #mel spectrogram\n        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n                                                           n_fft=n_fft,\n                                                           f_min=f_min,\n                                                           f_max=f_max,\n                                                           n_mels=num_mels) \n        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()  #cepstral coefficients\n        self.input_bn = nn.BatchNorm2d(1)\n        \n        #convolutional layers\n        self.layer1 = Conv_2d(1, num_channels, pooling=(2,3))\n        self.layer2 = Conv_2d(num_channels, num_channels, pooling=(3,4))\n        self.layer3 = Conv_2d(num_channels, num_channels*2, pooling=(2,5))\n        self.layer4 = Conv_2d(num_channels*2, num_channels*2, pooling=(3,3))\n        self.layer5 = Conv_2d(num_channels*2, num_channels*4, pooling=(3,4))\n        \n        #dense layers\n        self.dense1 = nn.Linear(num_channels*4, num_channels*4)\n        self.dense_bn = nn.BatchNorm1d(num_channels*4)\n        self.dense2 = nn.Linear(num_channels*4, num_classes) #maybe try softmax here\n        self.dropout = nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n        \n        \n    def forward(self,wav):\n        #input processing\n        \n#         print(wav.shape)\n        out = self.melspec(wav)\n#         print(out.shape)\n#         copy = torch.clone(out)\n#         copy.to('cpu')\n#         save_test_spec(copy)\n        \n        out = self.amplitude_to_db(out)\n        #input batch\n#       out = out.unsqueeze(1)\n#         print(out.shape)\n#         out = torch.permute(out, (0,3,2,1))\n#         print(out.shape)\n        out = self.input_bn(out)\n#         print(out.shape)\n\n        #convlayers\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n\n        # reshape. (batch_size, num_channels, 1, 1) -> (batch_size, num_channels)\n        out = out.reshape(len(out), -1)\n\n        #dense layers\n        out = self.dense1(out)\n        out = self.dense_bn(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.dense2(out)\n        \n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:03:12.347860Z","iopub.execute_input":"2022-12-12T04:03:12.348232Z","iopub.status.idle":"2022-12-12T04:03:12.362473Z","shell.execute_reply.started":"2022-12-12T04:03:12.348199Z","shell.execute_reply":"2022-12-12T04:03:12.361371Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#transform for resnet18 model\n\nfrom PIL import Image\nfrom torchvision import transforms\n\n# input_image = Image.open(filename)\npreprocess = transforms.Compose([\n    transforms.Resize((224, 1249)),\n#     transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n# input_tensor = preprocess(input_image)\n# input_batch = input_tensor.unsqueeze(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets try resnet\n\nclass CNN_modified(nn.Module):\n    def __init__(self, num_channels=16,\n              sample_rate = 22050,\n              n_fft=1024,\n              f_min=0.0,\n              f_max=11025.0,\n              num_mels=128,\n              num_classes=10):\n        \n        super(CNN,self).__init__()\n        \n        #mel spectrogram\n        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n                                                           n_fft=n_fft,\n                                                           f_min=f_min,\n                                                           f_max=f_max,\n                                                           n_mels=num_mels) \n        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()  #cepstral coefficients\n        self.input_bn = nn.BatchNorm2d(1)\n        self.resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n        \n        #convolutional layers\n#         self.layer1 = Conv_2d(1, num_channels, pooling=(2,3))\n#         self.layer2 = Conv_2d(num_channels, num_channels, pooling=(3,4))\n#         self.layer3 = Conv_2d(num_channels, num_channels*2, pooling=(2,5))\n#         self.layer4 = Conv_2d(num_channels*2, num_channels*2, pooling=(3,3))\n#         self.layer5 = Conv_2d(num_channels*2, num_channels*4, pooling=(3,4))\n        self.layer1 = resnet_model()\n    \n        #dense layers\n#         self.dense1 = nn.Linear(num_channels*4, num_channels*4)\n#         self.dense_bn = nn.BatchNorm1d(num_channels*4)\n#         self.dense2 = nn.Linear(num_channels*4, num_classes) #maybe try softmax here\n#         self.dropout = nn.Dropout(0.5)\n#         self.relu = nn.ReLU()\n        \n        \n    def forward(self,wav):\n        #input processing\n        \n#         print(wav.shape)\n        out = self.melspec(wav)\n#         print(out.shape)\n#         copy = torch.clone(out)\n#         copy.to('cpu')\n#         save_test_spec(copy)\n        \n        #make sure to reshape to at least 224x224 and normalizing with suggested standard deviation and mean\n        \n        #WRITE THAT FUNC\n        \n        out = self.amplitude_to_db(out)\n        #input batch\n#       out = out.unsqueeze(1)\n#         print(out.shape)\n#         out = torch.permute(out, (0,3,2,1))\n#         print(out.shape)\n        out = self.input_bn(out)\n#         print(out.shape)\n        \n        \n        #convlayers\n        out = self.layer1(out)\n#         out = self.layer2(out)\n#         out = self.layer3(out)\n#         out = self.layer4(out)\n#         out = self.layer5(out)\n        \n        return out\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ncnn = CNN().to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\nvalid_losses = []\nnum_epochs = 50","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:03:15.289249Z","iopub.execute_input":"2022-12-12T04:03:15.289944Z","iopub.status.idle":"2022-12-12T04:03:18.434340Z","shell.execute_reply.started":"2022-12-12T04:03:15.289907Z","shell.execute_reply":"2022-12-12T04:03:18.433323Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#training\nfor epoch in range(num_epochs):\n    train_losses = []\n    \n    #Train\n    cnn.train()\n    for (wav, genre_index) in train_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n\n        #forward\n        out= cnn(wav)\n        loss= loss_function(out, genre_index)\n\n        #backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n\n        mean_train_losses = np.mean(train_losses)\n    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, mean_train_losses))\n    \n    \n    #validation\n    cnn.eval()\n    y_true = []\n    y_pred = []\n    valid_losses = []\n    \n    for wav, genre_index in valid_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n\n        #reshape and aggregate chunk-level predictions\n        b, c, t = wav.size()\n        \n        check = wav.view(-1,t)\n#         print(check.shape)\n        \n#         logits = cnn(wav.view(-1,t))\n        logits = cnn(wav)\n        logits = logits.view(b, c, -1).mean(dim=1)\n        loss = loss_function(logits, genre_index)\n        valid_losses.append(loss.item())\n        _, pred = torch.max(logits.data,1)\n\n\n        #append\n        y_true.extend(genre_index.tolist())\n        y_pred.extend(pred.tolist())\n    accuracy = accuracy_score(y_true, y_pred)\n    valid_loss = np.mean(valid_losses)\n    print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy %.4f' % (epoch+1, num_epochs, valid_loss, accuracy))\n        \n    #save model\n    valid_losses.append(valid_loss.item())\n    if np.argmin(valid_losses) == epoch:\n        print('Saving the best model at %d epochs!' % epoch)\n        torch.save(cnn.state_dict(), 'best_model.ckpt')\n    \n#     wandb.log({\"train_loss\": mean_train_losses, \"valid_loss\":valid_loss, \"accuracy\":accuracy})\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:39:09.096048Z","iopub.execute_input":"2022-12-12T03:39:09.096730Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"torch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\nbad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\nbad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\nbad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"bad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([11, 1, 639450])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\nEpoch: [1/100], Train loss: 2.3440\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([5, 1, 639450])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\nEpoch: [1/100], Valid loss: 2.3514, Valid accuracy 0.1066\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"bad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([11, 1, 639450])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\nEpoch: [2/100], Train loss: 2.3525\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([5, 1, 639450])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\nEpoch: [2/100], Valid loss: 2.2124, Valid accuracy 0.1980\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"bad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([11, 1, 639450])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\nEpoch: [3/100], Train loss: 2.2748\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([5, 1, 639450])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\nEpoch: [3/100], Valid loss: 2.1354, Valid accuracy 0.2132\nSaving the best model at 2 epochs!\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"bad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([11, 1, 639450])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\ntorch.Size([11, 1, 128, 1249])\nEpoch: [4/100], Train loss: 2.2085\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([5, 1, 639450])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\ntorch.Size([5, 1, 128, 1249])\nEpoch: [4/100], Valid loss: 2.0694, Valid accuracy 0.2386\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\nbad file; return buffer file to batch\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 639450])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\ntorch.Size([16, 1, 128, 1249])\n","output_type":"stream"}]},{"cell_type":"code","source":"#evaluation\n\nS = torch.load('/kaggle/input/gtzanpreprocessed/best_model (50).ckpt')\ncnn.load_state_dict(S)\n\nprint('loaded!')\n\n#Run evaluation\n\ncnn.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for wav, genre_index in test_loader:\n        wav = wav.to(device)\n        genre_index = genre_index.to(device)\n        \n        #reshape and aggregate chunk- level predictions\n        b, c, t = wav.size()\n#         print(\"b,c,t\",b,c,t)\n#         logits = cnn(wav.view(-1, t))\n#         print(\"wav.view(-1,t).shape\",wav.view(-1,t).shape)\n        logits = cnn(wav)\n#         print(\"logits\", logits)\n        logits = logits.view(b, c, -1).mean(dim=1)\n#         print(\"logits data\",logits.data)\n        _, pred = torch.max(logits.data, 1)\n        print(\"predictions\", pred)\n        #append labels and predictions\n        y_true.extend(genre_index.tolist())\n        y_pred.extend(pred.tolist())\n#         print(y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:04:05.409992Z","iopub.execute_input":"2022-12-12T04:04:05.410467Z","iopub.status.idle":"2022-12-12T04:04:17.223471Z","shell.execute_reply.started":"2022-12-12T04:04:05.410429Z","shell.execute_reply":"2022-12-12T04:04:17.222332Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"loaded!\npredictions tensor([8, 8, 5, 8, 8, 8, 5, 8, 8, 5, 8, 8, 8, 8, 8, 5], device='cuda:0')\npredictions tensor([8, 6, 8, 4, 6, 3, 4, 4, 6, 5, 4, 6, 6, 6, 6, 1], device='cuda:0')\npredictions tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\npredictions tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 9], device='cuda:0')\npredictions tensor([1, 7, 4, 6, 1, 6, 6, 4, 4, 6, 1, 6, 2, 4, 6, 1], device='cuda:0')\npredictions tensor([6, 1, 6, 1, 9, 6, 8, 9, 1, 4, 8, 1, 4, 4, 4, 4], device='cuda:0')\npredictions tensor([4, 8, 4, 4, 6, 4, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')\npredictions tensor([4, 6, 4, 4, 4, 6, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4], device='cuda:0')\npredictions tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')\npredictions tensor([4, 4, 4, 4, 7, 5, 5, 2, 5, 0, 1, 6, 6, 6, 6, 1], device='cuda:0')\npredictions tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 6], device='cuda:0')\npredictions tensor([6, 6, 6, 6, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0')\npredictions tensor([6, 4, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 8, 6, 4, 6], device='cuda:0')\npredictions tensor([4, 4, 4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 4, 8, 4, 4], device='cuda:0')\npredictions tensor([4, 4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4], device='cuda:0')\npredictions tensor([4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, 5, 4, 4, 4], device='cuda:0')\npredictions tensor([4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 4, 4, 6, 6, 6], device='cuda:0')\npredictions tensor([6, 6, 6, 6, 4, 6, 6, 6, 9, 6, 6, 6, 6, 6, 6, 4], device='cuda:0')\npredictions tensor([3, 4], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# print(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, xticklabels=GTZAN_GENRES, yticklabels=GTZAN_GENRES, cmap='YlGnBu')\nprint('Accuracy: %.4f' % accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:04:43.664967Z","iopub.execute_input":"2022-12-12T04:04:43.665365Z","iopub.status.idle":"2022-12-12T04:04:44.555886Z","shell.execute_reply.started":"2022-12-12T04:04:43.665332Z","shell.execute_reply":"2022-12-12T04:04:44.554815Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 0.3034\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}